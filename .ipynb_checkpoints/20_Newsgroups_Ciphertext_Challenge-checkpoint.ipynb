{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import gc\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.svm import SVC\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    10024\n",
      "4     9970\n",
      "1     9589\n",
      "3     9469\n",
      "Name: difficulty, dtype: int64\n",
      "target       0    1    2    3    4    5    6    7    8    9    10   11   12  \\\n",
      "difficulty                                                                    \n",
      "1           420  465  360  346  320  466  361  331  380  486  540  563  344   \n",
      "2           455  528  652  391  315  550  310  405  387  301  580  651  396   \n",
      "3           394  567  695  386  313  524  293  422  331  460  479  534  338   \n",
      "4           465  366  940  382  326  653  272  437  386  413  437  622  429   \n",
      "\n",
      "target       13   14   15   16   17   18   19  \n",
      "difficulty                                     \n",
      "1           482  576  765  471  834  675  404  \n",
      "2           500  566  692  536  912  547  350  \n",
      "3           593  470  513  506  714  553  384  \n",
      "4           599  463  582  589  693  539  377  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "c     53752\n",
       "1    505426\n",
       "|      7490\n",
       "F      6134\n",
       "a    121319\n",
       "A     58440\n",
       "O    134813\n",
       "2      8502\n",
       "0     75960\n",
       "'     34490\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze the data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# load the dataframe\n",
    "df = pd.read_csv('20-newsgroups-ciphertext-challenge/train.csv')\n",
    "#test = pd.read_csv('20-newsgroups-ciphertext-challenge/test.csv')\n",
    "\n",
    "# dataframe for only difficulty=1\n",
    "data1 = df.query('difficulty==1')\n",
    "# how many elements do have diffuculty class\n",
    "print(df['difficulty'].value_counts()) \n",
    "# count repeated chars in ciphertext class\n",
    "alp = pd.Series(Counter(''.join(data1['ciphertext'])))  \n",
    "print(pd.crosstab(df['difficulty'], df['target']))\n",
    "alp.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features for train:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "count_chars: 100%|██████████| 39052/39052 [00:09<00:00, 4323.13it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:06<00:00, 5994.40it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:05<00:00, 7667.39it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:01<00:00, 32674.29it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:00<00:00, 339877.12it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:06<00:00, 5800.36it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:04<00:00, 7963.75it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:01<00:00, 32945.49it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:00<00:00, 594070.61it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:07<00:00, 5549.92it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:04<00:00, 8632.10it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:01<00:00, 34764.83it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:00<00:00, 613254.41it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:06<00:00, 6188.96it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:04<00:00, 8141.06it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:01<00:00, 34240.53it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:00<00:00, 555007.25it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:06<00:00, 6324.34it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:04<00:00, 8091.41it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:01<00:00, 33223.52it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:00<00:00, 559030.58it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:01<00:00, 23916.42it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:01<00:00, 30142.55it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:00<00:00, 90537.07it/s]\n",
      "strstat: 100%|██████████| 39052/39052 [00:28<00:00, 1382.72it/s]\n",
      "str_digit_stat: 100%|██████████| 39052/39052 [00:27<00:00, 1425.88it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>ciphertext</th>\n",
       "      <th>target</th>\n",
       "      <th>nunique</th>\n",
       "      <th>len</th>\n",
       "      <th>n_l</th>\n",
       "      <th>n_n</th>\n",
       "      <th>n_s</th>\n",
       "      <th>n_ul</th>\n",
       "      <th>...</th>\n",
       "      <th>str_max</th>\n",
       "      <th>str_skew</th>\n",
       "      <th>str_kurtosis</th>\n",
       "      <th>str_digit_sum</th>\n",
       "      <th>str_digit_mean</th>\n",
       "      <th>str_digit_std</th>\n",
       "      <th>str_digit_min</th>\n",
       "      <th>str_digit_max</th>\n",
       "      <th>str_digit_skew</th>\n",
       "      <th>str_digit_kurtosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_88b9bbd73</td>\n",
       "      <td>4</td>\n",
       "      <td>ob|I\u0006\f",
       "K?zzhX*L{83B3Z,\u0006FuL*Pusm$83L\\t@r$$*38,8s...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>300</td>\n",
       "      <td>0.473333</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.356667</td>\n",
       "      <td>0.246667</td>\n",
       "      <td>...</td>\n",
       "      <td>127.0</td>\n",
       "      <td>-0.091650</td>\n",
       "      <td>-0.950683</td>\n",
       "      <td>2664.0</td>\n",
       "      <td>52.235294</td>\n",
       "      <td>3.299366</td>\n",
       "      <td>48.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>-0.001394</td>\n",
       "      <td>-1.549887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_f489bd59f</td>\n",
       "      <td>1</td>\n",
       "      <td>c1|FaAO120O'8o\u0002vfoy1W#at\u001bvGs1[1s1[1/1]O-a8o1-\u001b...</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>300</td>\n",
       "      <td>0.383333</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.376667</td>\n",
       "      <td>0.103333</td>\n",
       "      <td>...</td>\n",
       "      <td>124.0</td>\n",
       "      <td>-0.047954</td>\n",
       "      <td>-1.144330</td>\n",
       "      <td>3568.0</td>\n",
       "      <td>49.555556</td>\n",
       "      <td>2.146631</td>\n",
       "      <td>48.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>2.564841</td>\n",
       "      <td>4.938975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_f90fee9c7</td>\n",
       "      <td>2</td>\n",
       "      <td>1*e4N8$f$0ccOui\u0018hkHek\u001a$\u0010k*\u001aV*hoe\u0010V\u001a$Hj8\bV\u0003hH8...</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>300</td>\n",
       "      <td>0.430000</td>\n",
       "      <td>0.146667</td>\n",
       "      <td>0.423333</td>\n",
       "      <td>0.176667</td>\n",
       "      <td>...</td>\n",
       "      <td>127.0</td>\n",
       "      <td>-0.063138</td>\n",
       "      <td>-1.231967</td>\n",
       "      <td>2406.0</td>\n",
       "      <td>54.681818</td>\n",
       "      <td>2.274091</td>\n",
       "      <td>48.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>-1.587334</td>\n",
       "      <td>1.299930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_8303ced65</td>\n",
       "      <td>1</td>\n",
       "      <td>O8v^10\u001bO#to1'#^'^\u0002\u001b\u0002tv1^]s111t0\u001b1O\u0002taq&gt;\u001b-ata_1...</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>300</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.246667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.153333</td>\n",
       "      <td>...</td>\n",
       "      <td>125.0</td>\n",
       "      <td>-0.077411</td>\n",
       "      <td>-0.983794</td>\n",
       "      <td>3682.0</td>\n",
       "      <td>49.756757</td>\n",
       "      <td>2.276857</td>\n",
       "      <td>48.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>2.245122</td>\n",
       "      <td>3.317398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_72abc2cb7</td>\n",
       "      <td>2</td>\n",
       "      <td>e\u001aV}H\u001a}kh\u001afe4b8'S.Vc}{A\f",
       "\f",
       ".#VikV.\u001afV?{$f7\u001a$Hjb8...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>300</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.113333</td>\n",
       "      <td>0.453333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>...</td>\n",
       "      <td>126.0</td>\n",
       "      <td>-0.241807</td>\n",
       "      <td>-1.200268</td>\n",
       "      <td>1875.0</td>\n",
       "      <td>55.147059</td>\n",
       "      <td>1.477923</td>\n",
       "      <td>51.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>-1.675256</td>\n",
       "      <td>1.324605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Id  difficulty  \\\n",
       "0  ID_88b9bbd73           4   \n",
       "1  ID_f489bd59f           1   \n",
       "2  ID_f90fee9c7           2   \n",
       "3  ID_8303ced65           1   \n",
       "4  ID_72abc2cb7           2   \n",
       "\n",
       "                                          ciphertext  target  nunique  len  \\\n",
       "0  ob|I\u0006\n",
       "K?zzhX*L{83B3Z,\u0006FuL*Pusm$83L\\t@r$$*38,8s...      10        1  300   \n",
       "1  c1|FaAO120O'8o\u0002vfoy1W#at\u001bvGs1[1s1[1/1]O-a8o1-\u001b...      13        1  300   \n",
       "2  1*e4N8$f$0ccOui\u0018hkHek\u001a$\u0010k*\u001aV*hoe\u0010V\u001a$Hj8\bV\u0003hH8...      19        1  300   \n",
       "3  O8v^10\u001bO#to1'#^'^\u0002\u001b\u0002tv1^]s111t0\u001b1O\u0002taq>\u001b-ata_1...      17        1  300   \n",
       "4  e\u001aV}H\u001a}kh\u001afe4b8'S.Vc}{A\n",
       "\n",
       ".#VikV.\u001afV?{$f7\u001a$Hjb8...       0        1  300   \n",
       "\n",
       "        n_l       n_n       n_s      n_ul         ...          str_max  \\\n",
       "0  0.473333  0.170000  0.356667  0.246667         ...            127.0   \n",
       "1  0.383333  0.240000  0.376667  0.103333         ...            124.0   \n",
       "2  0.430000  0.146667  0.423333  0.176667         ...            127.0   \n",
       "3  0.420000  0.246667  0.333333  0.153333         ...            125.0   \n",
       "4  0.433333  0.113333  0.453333  0.133333         ...            126.0   \n",
       "\n",
       "   str_skew  str_kurtosis  str_digit_sum  str_digit_mean  str_digit_std  \\\n",
       "0 -0.091650     -0.950683         2664.0       52.235294       3.299366   \n",
       "1 -0.047954     -1.144330         3568.0       49.555556       2.146631   \n",
       "2 -0.063138     -1.231967         2406.0       54.681818       2.274091   \n",
       "3 -0.077411     -0.983794         3682.0       49.756757       2.276857   \n",
       "4 -0.241807     -1.200268         1875.0       55.147059       1.477923   \n",
       "\n",
       "   str_digit_min  str_digit_max  str_digit_skew  str_digit_kurtosis  \n",
       "0           48.0           57.0       -0.001394           -1.549887  \n",
       "1           48.0           56.0        2.564841            4.938975  \n",
       "2           48.0           57.0       -1.587334            1.299930  \n",
       "3           48.0           56.0        2.245122            3.317398  \n",
       "4           51.0           56.0       -1.675256            1.324605  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature extraction\n",
    "import datetime\n",
    "import gc\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.stats import skew, kurtosis\n",
    "import lightgbm as lgb\n",
    "\n",
    "import Levenshtein\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "train = pd.read_csv('20-newsgroups-ciphertext-challenge/train.csv')\n",
    "data_1 = train.query('difficulty==1')\n",
    "X = data_1.iloc[:,:-1]\n",
    "y = data_1.iloc[:,-1]\n",
    "\n",
    "'''\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(trainDF['text'])\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(trainDF['text'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "\n",
    "ensemble.RandomForestClassifier()\n",
    "\n",
    "diff1['ciphertext'] = diff1['ciphertext'].apply(lambda x: x.replace('1', ' '))\n",
    "diff2['ciphertext'] = diff2['ciphertext'].apply(lambda x: x.replace('8', ' '))\n",
    "diff3['ciphertext'] = diff3['ciphertext'].apply(lambda x: x.replace('8', ' '))\n",
    "diff4['ciphertext'] = diff4['ciphertext'].apply(lambda x: x.replace('8', ' '))\n",
    "'''\n",
    "\n",
    "def extract_features(df):\n",
    "    #df['nunique'] = df['ciphertext'].apply(lambda x: len(np.unique(x)))\n",
    "    #df['len'] = df['ciphertext'].apply(lambda x: len(x))\n",
    "\n",
    "    def count_chars(x):\n",
    "        n_l = 0 # count letters\n",
    "        n_n = 0 # count numbers\n",
    "        n_s = 0 # count symbols\n",
    "        n_ul = 0 # count upper letters\n",
    "        n_ll = 0 # count lower letters\n",
    "        for i in range(0, len(x)):\n",
    "            if x[i].isalpha():\n",
    "                n_l += 1\n",
    "                if x[i].isupper():\n",
    "                    n_ul += 1\n",
    "                elif x[i].islower():\n",
    "                    n_ll += 1\n",
    "            elif x[i].isdigit():\n",
    "                n_n += 1\n",
    "            else:\n",
    "                n_s += 1\n",
    "\n",
    "        return pd.Series([n_l, n_n, n_s, n_ul, n_ll])\n",
    "\n",
    "    cols = ['n_l', 'n_n', 'n_s', 'n_ul', 'n_ll']\n",
    "    for c in cols:\n",
    "        df[c] = 0\n",
    "    tqdm.pandas(desc='count_chars')\n",
    "    df[cols] = df['ciphertext'].progress_apply(lambda x: count_chars(x))\n",
    "    for c in cols:\n",
    "        df[c] /= df['len']\n",
    "\n",
    "    tqdm.pandas(desc='distances')\n",
    "    df['Levenshtein_distance'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.distance(x, x[::-1]))\n",
    "    df['Levenshtein_ratio'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.ratio(x, x[::-1]))\n",
    "    df['Levenshtein_jaro'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.jaro(x, x[::-1]))\n",
    "    df['Levenshtein_hamming'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.hamming(x, x[::-1]))\n",
    "\n",
    "    for m in range(1, 5):\n",
    "        df['Levenshtein_distance_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.distance(x[:-m], x[m:]))\n",
    "        df['Levenshtein_ratio_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.ratio(x[:-m], x[m:]))\n",
    "        df['Levenshtein_jaro_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.jaro(x[:-m], x[m:]))\n",
    "        df['Levenshtein_hamming_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.hamming(x[:-m], x[m:]))\n",
    "    \n",
    "    df['Levenshtein_distance_h'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.distance(x[:len(x)//2], x[len(x)//2:]))\n",
    "    df['Levenshtein_ratio_h'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.ratio(x[:len(x)//2], x[len(x)//2:]))\n",
    "    df['Levenshtein_jaro_h'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.jaro(x[:len(x)//2], x[len(x)//2:]))\n",
    "    \n",
    "    # All symbols stats\n",
    "    def strstat(x):\n",
    "        r = np.array([ord(c) for c in x])\n",
    "        return pd.Series([\n",
    "            np.sum(r), \n",
    "            np.mean(r), \n",
    "            np.std(r), \n",
    "            np.min(r), \n",
    "            np.max(r),\n",
    "            skew(r), \n",
    "            kurtosis(r),\n",
    "            ])\n",
    "    cols = ['str_sum', 'str_mean', 'str_std', 'str_min', 'str_max', 'str_skew', 'str_kurtosis']\n",
    "    for c in cols:\n",
    "        df[c] = 0\n",
    "    tqdm.pandas(desc='strstat')\n",
    "    df[cols] = df['ciphertext'].progress_apply(lambda x: strstat(x))\n",
    "    \n",
    "    # Digit stats\n",
    "    def str_digit_stat(x):\n",
    "        r = np.array([ord(c) for c in x if c.isdigit()])\n",
    "        if len(r) == 0:\n",
    "            r = np.array([0])\n",
    "        return pd.Series([\n",
    "            np.sum(r), \n",
    "            np.mean(r), \n",
    "            np.std(r), \n",
    "            np.min(r), \n",
    "            np.max(r),\n",
    "            skew(r), \n",
    "            kurtosis(r),\n",
    "            ])\n",
    "    cols = ['str_digit_sum', 'str_digit_mean', 'str_digit_std', 'str_digit_min', \n",
    "        'str_digit_max', 'str_digit_skew', 'str_digit_kurtosis']\n",
    "    for c in cols:\n",
    "        df[c] = 0\n",
    "    tqdm.pandas(desc='str_digit_stat')\n",
    "    df[cols] = df['ciphertext'].progress_apply(lambda x: str_digit_stat(x))\n",
    "\n",
    "print('Extracting features for train:')\n",
    "extract_features(train)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2716371220020855\n"
     ]
    }
   ],
   "source": [
    "# Navier-Bayes modeli\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df = pd.read_csv('20-newsgroups-ciphertext-challenge/train.csv')\n",
    "data_1 = df.query('difficulty==1')\n",
    "X = data_1.iloc[:,-2]\n",
    "y = data_1.iloc[:,-1]\n",
    "\n",
    "def tokenize(text): \n",
    "    return text.split(\"1\")\n",
    "\n",
    "def trimm(text):\n",
    "    return ' '.join([i for i in text if len(i) > 1])\n",
    "\n",
    "token_data = [tokenize(i) for i in X]\n",
    "X = [trimm(i) for i in token_data]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify=y, random_state=0)\n",
    "\n",
    "nb = Pipeline([('vect', CountVectorizer(analyzer = 'word',lowercase = False,ngram_range=(1, 1))),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = nb.predict(X_test)\n",
    "print(accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5041710114702815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uzun/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# LogisticRegression classifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "df = pd.read_csv('20-newsgroups-ciphertext-challenge/train.csv')\n",
    "data_1 = df.query('difficulty==1')\n",
    "X = data_1.iloc[:,-2]\n",
    "y = data_1.iloc[:,-1]\n",
    "\n",
    "def tokenize(text): \n",
    "    return text.split(\"1\")\n",
    "\n",
    "def trimm(text):\n",
    "    return ' '.join([i for i in text if len(i) > 1])\n",
    "\n",
    "token_data = [tokenize(i) for i in X]\n",
    "X = [trimm(i) for i in token_data]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify=y, random_state=0)\n",
    "\n",
    "lr = Pipeline([('vect', CountVectorizer(analyzer = 'word',lowercase = False,ngram_range=(1, 1))),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', LogisticRegression(solver='lbfgs', n_jobs=1, C=1e5, multi_class='auto')),\n",
    "              ])\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "print(accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uzun/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5302398331595412\n"
     ]
    }
   ],
   "source": [
    "# SGDClassifier \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "df = pd.read_csv('20-newsgroups-ciphertext-challenge/train.csv')\n",
    "data_1 = df.query('difficulty==1')\n",
    "X = data_1.iloc[:,-2]\n",
    "y = data_1.iloc[:,-1]\n",
    "\n",
    "def tokenize(text): \n",
    "    return text.split(\"1\")\n",
    "\n",
    "def trimm(text):\n",
    "    return ' '.join([i for i in text if len(i) > 3])\n",
    "\n",
    "token_data = [tokenize(i) for i in X]\n",
    "#X = [trimm(i) for i in token_data]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify=y, random_state=0)\n",
    "\n",
    "svm = Pipeline([('vect', CountVectorizer(analyzer = 'word',tokenizer = tokenize,\n",
    "                                        lowercase = False,ngram_range=(1, 1))),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-5,\n",
    "                                     random_state=42, max_iter=5, tol=None)),])\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svm.predict(X_test)\n",
    "print(accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6768107786100701"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SGDClassifier with high accuracy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "df = pd.read_csv('20-newsgroups-ciphertext-challenge/train.csv')\n",
    "data_1 = df.query('difficulty==1')\n",
    "X = data_1.iloc[:,:-1]\n",
    "y = data_1.iloc[:,-1]\n",
    "\n",
    "def tokenize(text): \n",
    "    return text.split(\"1\")\n",
    "\n",
    "def trimm(text):\n",
    "    return ' '.join([i for i in text if len(i) > 3])\n",
    "\n",
    "token_data = [tokenize(i) for i in X]\n",
    "#X = [trimm(i) for i in token_data]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify=y, random_state=0)\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = 'char',tokenizer = tokenize,lowercase = False,ngram_range=(1, 6))\n",
    "\n",
    "estimator = SGDClassifier(loss='hinge', max_iter=1000, random_state=0,tol=1e-3, n_jobs=-1)\n",
    "\n",
    "model = Pipeline([('selector',FunctionTransformer(lambda x: x['ciphertext'], validate=False)),\n",
    "                  ('vectorizer', vectorizer), \n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('estimator', estimator)])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify=y, random_state=0)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': <gensim.models.keyedvectors.Vocab object at 0x7ff0b7b88f28>, 'A': <gensim.models.keyedvectors.Vocab object at 0x7ff0b7b88828>, 'O': <gensim.models.keyedvectors.Vocab object at 0x7ff0b7b88780>, ' ': <gensim.models.keyedvectors.Vocab object at 0x7ff0b7b88320>, '0': <gensim.models.keyedvectors.Vocab object at 0x7ff0b7b88e10>, \"'\": <gensim.models.keyedvectors.Vocab object at 0x7ff0b7b88e48>, '8': <gensim.models.keyedvectors.Vocab object at 0x7ff0b7b88e80>, 'o': <gensim.models.keyedvectors.Vocab object at 0x7ff0b7b88d68>, '\\x02': <gensim.models.keyedvectors.Vocab object at 0x7ff0b7b88f60>, 'v': <gensim.models.keyedvectors.Vocab object at 0x7ff0b7b88da0>, 'f': <gensim.models.keyedvectors.Vocab object at 0x7ff0b7b88ef0>, 'W': <gensim.models.keyedvectors.Vocab object at 0x7ff0b7b88d30>, '#': <gensim.models.keyedvectors.Vocab object at 0x7ff0b7b88cf8>, 't': <gensim.models.keyedvectors.Vocab object at 0x7ff0b7b88c88>, '\\x1b': <gensim.models.keyedvectors.Vocab object at 0x7ff0b7b88c50>, 's': <gensim.models.keyedvectors.Vocab object at 0x7ff0b7b88c18>, ']': <gensim.models.keyedvectors.Vocab object at 0x7ff0b7b88be0>, '-': <gensim.models.keyedvectors.Vocab object at 0x7ff0b7b88ba8>, '\\x03': <gensim.models.keyedvectors.Vocab object at 0x7ff0b7b88b38>, 'c': <gensim.models.keyedvectors.Vocab object at 0x7ff0b7b88b00>, 'd': <gensim.models.keyedvectors.Vocab object at 0x7ff0b7b88ac8>, '^': <gensim.models.keyedvectors.Vocab object at 0x7ff0b7b88a58>, 'z': <gensim.models.keyedvectors.Vocab object at 0x7ff0b7b88a20>, '\\x08': <gensim.models.keyedvectors.Vocab object at 0x7ff0b7b889b0>, '_': <gensim.models.keyedvectors.Vocab object at 0x7ff0b7b88978>}\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df = pd.read_csv('20-newsgroups-ciphertext-challenge/train.csv')\n",
    "data_1 = df.query('difficulty==1')\n",
    "X = data_1.iloc[:,-2]\n",
    "y = data_1.iloc[:,-1]\n",
    "\n",
    "X.apply(lambda x: x[:6]).value_counts().reset_index().head(10)\n",
    "\n",
    "def tokenize(text): \n",
    "    return text.split(\"1\")\n",
    "\n",
    "def trimm(text):\n",
    "    return ' '.join([i for i in text if len(i) > 3])\n",
    "\n",
    "token_data = [tokenize(i) for i in X]\n",
    "X = [trimm(i) for i in token_data]\n",
    "\n",
    "word2vec = Word2Vec(X, min_count=2)  \n",
    "vocabulary = word2vec.wv.vocab  \n",
    "print(vocabulary)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-84fc263b2544>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0mxgb_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.075\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolsample_bytree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0mxgb_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_svd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The model is ready.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    711\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1110\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# xgboost olan kod\n",
    "X = df['ciphertext']\n",
    "y = df['target']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[:10000], y[:10000], test_size=0.25)\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "import nltk\n",
    "\n",
    "def Tokenizer(str_input):\n",
    "    str_input = str_input.lower()\n",
    "    words = word_tokenize(str_input)\n",
    "    #remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    #stem the words\n",
    "    porter_stemmer=nltk.PorterStemmer()\n",
    "    words = [porter_stemmer.stem(word) for word in words]\n",
    "    return words\n",
    "\n",
    "# BURAYA BAAAAAAAAAAAAAAAAAAAAAAAAK\n",
    "data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from xgboost import XGBClassifier\n",
    "'''\n",
    "print(\"Pipelining..\")\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(tokenizer=Tokenizer, max_df=0.3, min_df=0.001, max_features=100000)),\n",
    "    #('svd',   TruncatedSVD(algorithm='randomized', n_components=500)),\n",
    "    ('clf',   XGBClassifier(objective='multi:softmax', n_estimators=500, num_class=20, learning_rate=0.075, colsample_bytree=0.7, subsample=0.8, eval_metric='merror')),\n",
    "])\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {\n",
    "    #'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "    #'tfidf__min_df': (0.001, 0.0025, 0.005),\n",
    "    #'tfidf__max_features': (50000, 100000, 150000),\n",
    "    #'tfidf__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    #'tfidf__use_idf': (True, False),\n",
    "    # 'tfidf__norm': ('l1', 'l2'),\n",
    "    #'svd__n_components': (250, 500, 750),\n",
    "    #'clf__n_estimators': (250, 500, 750),\n",
    "    'clf__max_depth': (4, 6, 8),\n",
    "    'clf__min_child_weight': (1, 5, 10),\n",
    "    #'clf__alpha': (0.00001, 0.000001),\n",
    "    #'clf__penalty': ('l2', 'elasticnet'),\n",
    "    #'clf__max_iter': (10, 50, 80),\n",
    "}\n",
    "\n",
    "#gs_clf = GridSearchCV(text_clf, parameters, cv=5, iid=False, n_jobs=-1)\n",
    "#gs_clf.fit(X_sample.message, y_sample)\n",
    "\n",
    "#print(\"Best score: %0.3f\" % gs_clf.best_score_)\n",
    "#print(\"Best parameters set:\")\n",
    "#best_parameters = gs_clf.best_estimator_.get_params()\n",
    "#for param_name in sorted(parameters.keys()):\n",
    "#    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "print(\"Fitting..\")\n",
    "\n",
    "text_clf.fit(X_train, y_train)\n",
    "predictions = text_clf.predict(X_test)\n",
    "print(\"The training predictions are ready\")\n",
    "'''\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=Tokenizer, min_df=0.001, max_df=0.3)\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "#vectorizer.get_feature_names()\n",
    "X_tfidf.shape\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd_transformer = TruncatedSVD(algorithm='randomized', n_components=300)\n",
    "X_svd = svd_transformer.fit_transform(X_tfidf)\n",
    "X_svd.shape\n",
    "\n",
    "xgb_classifier = XGBClassifier(max_depth=3, n_estimators=1000, learning_rate=0.075, colsample_bytree=0.7, subsample=0.8)\n",
    "xgb_classifier.fit(X_svd, y)\n",
    "print(\"The model is ready.\")\n",
    "\n",
    "X_test_tfidf = vectorizer.transform(X_test.ciphertext)\n",
    "X_test_svd = svd_transformer.transform(X_test_tfidf)\n",
    "\n",
    "xgb_predictions = xgb_classifier.predict(X_test_svd)\n",
    "predictions = xgb_predictions\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, classification_report, confusion_matrix\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(\"Precision:\", precision_score(y_test, predictions, average='weighted'))\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "# feature visualize\n",
    "#xgb.plot_importance(xg_reg)\n",
    "#plt.rcParams['figure.figsize'] = [5, 5]\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.29 µs\n",
      "Modeling..\n",
      "0.9406003159557662\n",
      "Auc Score:  0.5225776259649573\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96       135\n",
      "           1       0.99      0.91      0.95       157\n",
      "           2       0.84      0.91      0.87       107\n",
      "           3       0.89      0.96      0.93       109\n",
      "           4       0.83      0.90      0.86        89\n",
      "           5       0.95      0.89      0.92       150\n",
      "           6       0.89      0.92      0.90       118\n",
      "           7       0.89      0.96      0.92       114\n",
      "           8       0.82      0.95      0.88       128\n",
      "           9       0.97      0.97      0.97       158\n",
      "          10       0.99      0.99      0.99       169\n",
      "          11       1.00      0.94      0.97       200\n",
      "          12       0.72      0.92      0.81       113\n",
      "          13       0.96      0.92      0.94       163\n",
      "          14       0.98      0.91      0.95       198\n",
      "          15       1.00      0.98      0.99       246\n",
      "          16       0.99      0.90      0.94       173\n",
      "          17       1.00      1.00      1.00       257\n",
      "          18       0.97      0.93      0.95       243\n",
      "          19       0.93      0.93      0.93       138\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      3165\n",
      "   macro avg       0.93      0.94      0.93      3165\n",
      "weighted avg       0.95      0.94      0.94      3165\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import re\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes, linear_model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "#import lightgbm as lgb\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import warnings\n",
    "from nltk.corpus import stopwords\n",
    "stops = stopwords.words(\"english\")\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss,confusion_matrix,classification_report,roc_curve,auc\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('20-newsgroups-ciphertext-challenge/train.csv')\n",
    "data_1 = df.query('difficulty==1')\n",
    "X = data_1.iloc[:,-2]\n",
    "y = data_1.iloc[:,-1]\n",
    "\n",
    "X = X.str.lower()\n",
    "X = X.astype(str)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify=y, random_state=0)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf = TfidfVectorizer(token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b')\n",
    "\n",
    "# build TFIDF Vectorizer\n",
    "tokens= ((u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b')) # makes sure it matches a word but contains at least one letter\n",
    "\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    #stop_words = 'english',\n",
    "    strip_accents='ascii',\n",
    "    analyzer='word',\n",
    "    token_pattern=tokens,\n",
    "    ngram_range=(1,2),\n",
    "    dtype=np.float32,\n",
    "    max_features=7500\n",
    ")\n",
    "\n",
    "# Character Stemmer\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    #stop_words = 'english',\n",
    "    strip_accents='ascii',\n",
    "    analyzer='char',\n",
    "    token_pattern=tokens,\n",
    "    ngram_range=(2, 4),\n",
    "    dtype=np.float32,\n",
    "    max_features=12000\n",
    ")\n",
    "\n",
    "word_vectorizer.fit(X)\n",
    "char_vectorizer.fit(X)\n",
    "\n",
    "train_word_features = word_vectorizer.transform(X)\n",
    "train_char_features = char_vectorizer.transform(X)\n",
    "\n",
    "train_features = hstack([train_char_features,train_word_features])\n",
    "\n",
    "Target = y\n",
    "\n",
    "%time\n",
    "print(\"Modeling..\")\n",
    "loss = []\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(train_features, Target, test_size=0.33)\n",
    "\n",
    "lr = LogisticRegression(solver=\"sag\", max_iter=100,class_weight='balanced',C=2.65,penalty='l2')\n",
    "lr.fit(train_features,Target)\n",
    "lr_pred=lr.predict(X_test_tfidf)\n",
    "\n",
    "accuracy_tfidf =accuracy_score(y_test_tfidf,lr_pred)\n",
    "\n",
    "print(accuracy_tfidf)\n",
    "print(\"Auc Score: \",np.mean(cross_val_score(lr, train_features, Target, cv=3,)))\n",
    "\n",
    "print(classification_report(y_test_tfidf,lr_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
