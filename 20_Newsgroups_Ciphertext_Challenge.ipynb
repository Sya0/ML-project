{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import gc\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.svm import SVC\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    10024\n",
      "4     9970\n",
      "1     9589\n",
      "3     9469\n",
      "Name: difficulty, dtype: int64\n",
      "target       0    1    2    3    4    5    6    7    8    9    10   11   12  \\\n",
      "difficulty                                                                    \n",
      "1           420  465  360  346  320  466  361  331  380  486  540  563  344   \n",
      "2           455  528  652  391  315  550  310  405  387  301  580  651  396   \n",
      "3           394  567  695  386  313  524  293  422  331  460  479  534  338   \n",
      "4           465  366  940  382  326  653  272  437  386  413  437  622  429   \n",
      "\n",
      "target       13   14   15   16   17   18   19  \n",
      "difficulty                                     \n",
      "1           482  576  765  471  834  675  404  \n",
      "2           500  566  692  536  912  547  350  \n",
      "3           593  470  513  506  714  553  384  \n",
      "4           599  463  582  589  693  539  377  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "c     53752\n",
       "1    505426\n",
       "|      7490\n",
       "F      6134\n",
       "a    121319\n",
       "A     58440\n",
       "O    134813\n",
       "2      8502\n",
       "0     75960\n",
       "'     34490\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze the data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# load the dataframe\n",
    "df = pd.read_csv('20-newsgroups-ciphertext-challenge/train.csv')\n",
    "#test = pd.read_csv('20-newsgroups-ciphertext-challenge/test.csv')\n",
    "\n",
    "# dataframe for only difficulty=1\n",
    "data1 = df.query('difficulty==1')\n",
    "# how many elements do have diffuculty class\n",
    "print(df['difficulty'].value_counts()) \n",
    "# count repeated chars in ciphertext class\n",
    "alp = pd.Series(Counter(''.join(data1['ciphertext'])))  \n",
    "print(pd.crosstab(df['difficulty'], df['target']))\n",
    "alp.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2716371220020855\n"
     ]
    }
   ],
   "source": [
    "# Navier-Bayes modeli\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df = pd.read_csv('20-newsgroups-ciphertext-challenge/train.csv')\n",
    "data_1 = df.query('difficulty==1')\n",
    "X = data_1.iloc[:,-2]\n",
    "y = data_1.iloc[:,-1]\n",
    "\n",
    "def tokenize(text): \n",
    "    return text.split(\"1\")\n",
    "\n",
    "def trimm(text):\n",
    "    return ' '.join([i for i in text if len(i) > 1])\n",
    "\n",
    "token_data = [tokenize(i) for i in X]\n",
    "X = [trimm(i) for i in token_data]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify=y, random_state=0)\n",
    "# trigger timer\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "# create and train model\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "for train, test in skf.split(X, y):\n",
    "\n",
    "nb = Pipeline([('vect', CountVectorizer(analyzer = 'word',lowercase = False,ngram_range=(1, 1))),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = nb.predict(X_test)\n",
    "print(accuracy_score(y_pred, y_test))\n",
    "# calculate time interval\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(\"Elapsed time\", elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5041710114702815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uzun/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# LogisticRegression classifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "df = pd.read_csv('20-newsgroups-ciphertext-challenge/train.csv')\n",
    "data_1 = df.query('difficulty==1')\n",
    "X = data_1.iloc[:,-2]\n",
    "y = data_1.iloc[:,-1]\n",
    "\n",
    "def tokenize(text): \n",
    "    return text.split(\"1\")\n",
    "\n",
    "def trimm(text):\n",
    "    return ' '.join([i for i in text if len(i) > 1])\n",
    "\n",
    "token_data = [tokenize(i) for i in X]\n",
    "X = [trimm(i) for i in token_data]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify=y, random_state=0)\n",
    "\n",
    "lr = Pipeline([('vect', CountVectorizer(analyzer = 'word',lowercase = False,ngram_range=(1, 1))),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', LogisticRegression(solver='lbfgs', n_jobs=1, C=1e5, multi_class='auto')),\n",
    "              ])\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "print(accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uzun/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5302398331595412\n"
     ]
    }
   ],
   "source": [
    "# SGDClassifier \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "df = pd.read_csv('20-newsgroups-ciphertext-challenge/train.csv')\n",
    "data_1 = df.query('difficulty==1')\n",
    "X = data_1.iloc[:,-2]\n",
    "y = data_1.iloc[:,-1]\n",
    "\n",
    "def tokenize(text): \n",
    "    return text.split(\"1\")\n",
    "\n",
    "def trimm(text):\n",
    "    return ' '.join([i for i in text if len(i) > 3])\n",
    "\n",
    "token_data = [tokenize(i) for i in X]\n",
    "#X = [trimm(i) for i in token_data]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify=y, random_state=0)\n",
    "\n",
    "svm = Pipeline([('vect', CountVectorizer(analyzer = 'word',tokenizer = tokenize,\n",
    "                                        lowercase = False,ngram_range=(1, 1))),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-5,\n",
    "                                     random_state=42, max_iter=5, tol=None)),])\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svm.predict(X_test)\n",
    "print(accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [3, 9589]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c449d62628fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrimm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalyzer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'char'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlowercase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2182\u001b[0m         \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2184\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2186\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 235\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [3, 9589]"
     ]
    }
   ],
   "source": [
    "# SGDClassifier with high accuracy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('20-newsgroups-ciphertext-challenge/train.csv')\n",
    "data_1 = df.query('difficulty==1')\n",
    "X = data_1.iloc[:,:-1]\n",
    "y = data_1.iloc[:,-1]\n",
    "\n",
    "def tokenize(text): \n",
    "    return text.split(\"1\")\n",
    "\n",
    "def trimm(text):\n",
    "    return ' '.join([i for i in text if len(i) > 3])\n",
    "\n",
    "token_data = [tokenize(i) for i in X]\n",
    "#X = [trimm(i) for i in token_data]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify=y, random_state=0)\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = 'char',tokenizer = tokenize,lowercase = False,ngram_range=(1, 6))\n",
    "\n",
    "estimator = SGDClassifier(loss='hinge', max_iter=1000, random_state=0,tol=1e-3, n_jobs=-1)\n",
    "\n",
    "model = Pipeline([('selector',FunctionTransformer(lambda x: x['ciphertext'], validate=False)),\n",
    "                  ('vectorizer', vectorizer), \n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('estimator', estimator)])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "Modeling..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uzun/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/uzun/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9169036334913112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uzun/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/uzun/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auc Score:  0.5028861672903329\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.95      0.84       147\n",
      "           1       0.93      0.95      0.94       150\n",
      "           2       0.83      0.89      0.86       117\n",
      "           3       0.85      0.80      0.82        99\n",
      "           4       0.65      0.97      0.78       103\n",
      "           5       0.93      0.87      0.90       163\n",
      "           6       0.96      0.92      0.94       119\n",
      "           7       0.88      0.88      0.88       112\n",
      "           8       0.89      0.93      0.91       126\n",
      "           9       0.99      0.94      0.96       142\n",
      "          10       0.99      0.95      0.97       186\n",
      "          11       1.00      0.90      0.94       200\n",
      "          12       0.75      0.94      0.84       111\n",
      "          13       0.99      0.94      0.96       160\n",
      "          14       0.98      0.86      0.92       185\n",
      "          15       0.99      0.94      0.96       244\n",
      "          16       0.91      0.93      0.92       134\n",
      "          17       0.99      0.96      0.98       286\n",
      "          18       0.97      0.94      0.96       241\n",
      "          19       0.94      0.81      0.87       140\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      3165\n",
      "   macro avg       0.91      0.91      0.91      3165\n",
      "weighted avg       0.93      0.92      0.92      3165\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import re\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes, linear_model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "#import lightgbm as lgb\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import warnings\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss,confusion_matrix,classification_report,roc_curve,auc\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('20-newsgroups-ciphertext-challenge/train.csv')\n",
    "data_1 = df.query('difficulty==1')\n",
    "X = data_1.iloc[:,-2]\n",
    "y = data_1.iloc[:,-1]\n",
    "\n",
    "#X = X.str.lower()\n",
    "X = X.astype(str)\n",
    "\n",
    "def tokenize(text): \n",
    "    return text.split(\"1\")\n",
    "\n",
    "def trimm(text):\n",
    "    return ' '.join([i for i in text if len(i) > 3])\n",
    "\n",
    "token_data = [tokenize(i) for i in X]\n",
    "X = [trimm(i) for i in token_data]\n",
    "\n",
    "tf = TfidfVectorizer(token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b')\n",
    "\n",
    "# build TFIDF Vectorizer\n",
    "tokens= ((u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b')) # makes sure it matches a word but contains at least one letter\n",
    "\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    #stop_words = 'english',\n",
    "    strip_accents='ascii',\n",
    "    analyzer='word',\n",
    "    token_pattern=tokens,\n",
    "    ngram_range=(1,2),\n",
    "    dtype=np.float32,\n",
    "    max_features=7500\n",
    ")\n",
    "\n",
    "# Character Stemmer\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    #stop_words = 'english',\n",
    "    strip_accents='ascii',\n",
    "    analyzer='char',\n",
    "    token_pattern=tokens,\n",
    "    ngram_range=(2, 4),\n",
    "    dtype=np.float32,\n",
    "    max_features=12000\n",
    ")\n",
    "\n",
    "word_vectorizer.fit(X)\n",
    "char_vectorizer.fit(X)\n",
    "\n",
    "train_word_features = word_vectorizer.transform(X)\n",
    "train_char_features = char_vectorizer.transform(X)\n",
    "\n",
    "train_features = hstack([train_char_features,train_word_features])\n",
    "\n",
    "%time\n",
    "print(\"Modeling..\")\n",
    "\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(train_features, y, test_size=0.33)\n",
    "\n",
    "lr = LogisticRegression(solver=\"sag\", max_iter=100,class_weight='balanced',C=2.65,penalty='l2')\n",
    "lr.fit(train_features,y)\n",
    "lr_pred=lr.predict(X_test_tfidf)\n",
    "\n",
    "accuracy_tfidf =accuracy_score(y_test_tfidf,lr_pred)\n",
    "\n",
    "print(accuracy_tfidf)\n",
    "print(\"Auc Score: \",np.mean(cross_val_score(lr, train_features, y, cv=3,)))\n",
    "\n",
    "print(classification_report(y_test_tfidf,lr_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipelining..\n",
      "The training predictions are ready\n",
      "Accuracy: 0.104\n",
      "Precision: 0.097928078817734\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        15\n",
      "           1       0.25      0.12      0.17         8\n",
      "           2       0.67      0.20      0.31        20\n",
      "           3       0.00      0.00      0.00         6\n",
      "           4       0.00      0.00      0.00         6\n",
      "           5       0.07      0.27      0.11        11\n",
      "           6       0.00      0.00      0.00        13\n",
      "           7       0.00      0.00      0.00         8\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00         6\n",
      "          10       0.00      0.00      0.00        18\n",
      "          11       0.09      0.21      0.12        14\n",
      "          12       0.00      0.00      0.00         6\n",
      "          13       0.07      0.12      0.09        16\n",
      "          14       0.00      0.00      0.00        11\n",
      "          15       0.21      0.19      0.20        16\n",
      "          16       0.00      0.00      0.00        19\n",
      "          17       0.11      0.42      0.18        24\n",
      "          18       0.00      0.00      0.00        17\n",
      "          19       0.00      0.00      0.00         8\n",
      "\n",
      "   micro avg       0.10      0.10      0.10       250\n",
      "   macro avg       0.07      0.08      0.06       250\n",
      "weighted avg       0.10      0.10      0.08       250\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom nltk.tokenize import word_tokenize\\nfrom sklearn.decomposition import TruncatedSVD\\n\\nvectorizer = TfidfVectorizer(tokenizer=Tokenizer, min_df=0.001, max_df=0.3)\\nX_tfidf = vectorizer.fit_transform(X)\\n#vectorizer.get_feature_names()\\n\\nsvd_transformer = TruncatedSVD(algorithm=\\'randomized\\', n_components=300)\\nX_svd = svd_transformer.fit_transform(X_tfidf)\\n\\nxgb_classifier = XGBClassifier()\\nxgb_classifier.fit(X_svd, y)\\nprint(\"The model is ready.\")\\n\\nX_test_tfidf = vectorizer.transform(X_test)\\nX_test_svd = svd_transformer.transform(X_test_tfidf)\\n\\nxgb_predictions = xgb_classifier.predict(X_test_svd)\\npredictions = xgb_predictions\\n\\nprint(\"Accuracy:\", accuracy_score(y_test, predictions))\\nprint(\"Precision:\", precision_score(y_test, predictions, average=\\'weighted\\'))\\nprint(classification_report(y_test, predictions))\\n\\n# feature visualize\\n#xgb.plot_importance(xg_reg)\\n#plt.rcParams[\\'figure.figsize\\'] = [5, 5]\\n#plt.show()\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xgboost olan kod\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, precision_score, classification_report, confusion_matrix\n",
    "\n",
    "df = pd.read_csv('20-newsgroups-ciphertext-challenge/train.csv')\n",
    "data_1 = df.query('difficulty==1')\n",
    "X = data_1.iloc[:,-2]\n",
    "y = data_1.iloc[:,-1]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[:1000], y[:1000], test_size=0.25)\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "import nltk\n",
    "\n",
    "tokens= ((u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b')) # makes sure it matches a word but contains at least one letter \n",
    "\n",
    "# BURAYA BAAAAAAAAAAAAAAAAAAAAAAAAK\n",
    "#data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "#cv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=3,\n",
    "#                    num_boost_round=50,early_stopping_rounds=10,metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "print(\"Pipelining..\")\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(sublinear_tf=True,strip_accents='ascii',analyzer='word',token_pattern=tokens,\n",
    "                              ngram_range=(1,2),dtype=np.float32,max_features=7500)),\n",
    "    ('svd',   TruncatedSVD(algorithm='randomized', n_components=500)),\n",
    "    ('clf',   XGBClassifier(objective='multi:softmax', n_estimators=500, num_class=20, learning_rate=0.075, \n",
    "                            colsample_bytree=0.7, subsample=0.8, eval_metric='merror')),\n",
    "])\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {\n",
    "    #'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "    #'tfidf__min_df': (0.001, 0.0025, 0.005),\n",
    "    #'tfidf__max_features': (50000, 100000, 150000),\n",
    "    #'tfidf__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    #'tfidf__use_idf': (True, False),\n",
    "    # 'tfidf__norm': ('l1', 'l2'),\n",
    "    #'svd__n_components': (250, 500, 750),\n",
    "    #'clf__n_estimators': (250, 500, 750),\n",
    "    'clf__max_depth': (4, 6, 8),\n",
    "    'clf__min_child_weight': (1, 5, 10),\n",
    "    #'clf__alpha': (0.00001, 0.000001),\n",
    "    #'clf__penalty': ('l2', 'elasticnet'),\n",
    "    #'clf__max_iter': (10, 50, 80),\n",
    "}\n",
    "\n",
    "#gs_clf = GridSearchCV(text_clf, parameters, cv=5, iid=False, n_jobs=-1)\n",
    "#gs_clf.fit(X_sample.message, y_sample)\n",
    "\n",
    "#print(\"Best score: %0.3f\" % gs_clf.best_score_)\n",
    "#print(\"Best parameters set:\")\n",
    "#best_parameters = gs_clf.best_estimator_.get_params()\n",
    "#for param_name in sorted(parameters.keys()):\n",
    "#    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "text_clf.fit(X_train, y_train)\n",
    "predictions = text_clf.predict(X_test)\n",
    "print(\"The training predictions are ready\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(\"Precision:\", precision_score(y_test, predictions, average='weighted'))\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "'''\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=Tokenizer, min_df=0.001, max_df=0.3)\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "#vectorizer.get_feature_names()\n",
    "\n",
    "svd_transformer = TruncatedSVD(algorithm='randomized', n_components=300)\n",
    "X_svd = svd_transformer.fit_transform(X_tfidf)\n",
    "\n",
    "xgb_classifier = XGBClassifier()\n",
    "xgb_classifier.fit(X_svd, y)\n",
    "print(\"The model is ready.\")\n",
    "\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "X_test_svd = svd_transformer.transform(X_test_tfidf)\n",
    "\n",
    "xgb_predictions = xgb_classifier.predict(X_test_svd)\n",
    "predictions = xgb_predictions\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(\"Precision:\", precision_score(y_test, predictions, average='weighted'))\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "# feature visualize\n",
    "#xgb.plot_importance(xg_reg)\n",
    "#plt.rcParams['figure.figsize'] = [5, 5]\n",
    "#plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4432860717264387\n",
      "Precision: 0.4917226646914811\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.20      0.27        99\n",
      "           1       0.43      0.45      0.44       112\n",
      "           2       0.62      0.39      0.48        90\n",
      "           3       0.41      0.24      0.30        88\n",
      "           4       0.44      0.13      0.21        82\n",
      "           5       0.38      0.37      0.37       104\n",
      "           6       0.52      0.49      0.51        91\n",
      "           7       0.62      0.08      0.14        98\n",
      "           8       0.59      0.39      0.47        84\n",
      "           9       0.64      0.49      0.56       124\n",
      "          10       0.50      0.63      0.56       144\n",
      "          11       0.57      0.54      0.55       142\n",
      "          12       0.74      0.15      0.25        91\n",
      "          13       0.75      0.27      0.39       146\n",
      "          14       0.55      0.62      0.58       141\n",
      "          15       0.34      0.82      0.48       184\n",
      "          16       0.54      0.23      0.33       111\n",
      "          17       0.34      0.83      0.48       201\n",
      "          18       0.41      0.54      0.47       163\n",
      "          19       0.19      0.03      0.05       103\n",
      "\n",
      "   micro avg       0.44      0.44      0.44      2398\n",
      "   macro avg       0.50      0.39      0.39      2398\n",
      "weighted avg       0.49      0.44      0.41      2398\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RandomForest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('20-newsgroups-ciphertext-challenge/train.csv')\n",
    "data_1 = df.query('difficulty==1')\n",
    "X = data_1.iloc[:,-2]\n",
    "y = data_1.iloc[:,-1]\n",
    "\n",
    "def tokenize(text): \n",
    "    return text.split(\"1\")\n",
    "\n",
    "def trimm(text):\n",
    "    return ' '.join([i for i in text if len(i) > 3])\n",
    "\n",
    "token_data = [tokenize(i) for i in X]\n",
    "X = [trimm(i) for i in token_data]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "tokens= ((u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b')) # makes sure it matches a word but contains at least one letter \n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(sublinear_tf=True,strip_accents='ascii',analyzer='word',token_pattern=tokens,\n",
    "                              ngram_range=(1,2),dtype=np.float32,max_features=7500)),\n",
    "    ('clf',   RandomForestClassifier(n_estimators=500,max_features='log2',min_samples_split=4)),\n",
    "])\n",
    "#fit_transform araştır!!!!!!!!!!!!!!!!\n",
    "\n",
    "text_clf.fit(X_train,y_train)\n",
    "predictions = text_clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(\"Precision:\", precision_score(y_test, predictions, average='weighted'))\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9589, 10)\n"
     ]
    }
   ],
   "source": [
    "# PCA \n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "df = pd.read_csv('20-newsgroups-ciphertext-challenge/train.csv')\n",
    "data_1 = df.query('difficulty==1')\n",
    "X = data_1.iloc[:,-2]\n",
    "y = data_1.iloc[:,-1]\n",
    "\n",
    "def tokenize(text): \n",
    "    return text.split(\"1\")\n",
    "\n",
    "def trimm(text):\n",
    "    return ' '.join([i for i in text if len(i) > 3])\n",
    "\n",
    "token_data = [tokenize(i) for i in X]\n",
    "X = [trimm(i) for i in token_data]\n",
    "\n",
    "word2vec = Word2Vec(X, min_count=2)  \n",
    "a=cosine_similarity(train_word_features, train_word_features)\n",
    "#vocabulary = word2vec.wv.vocab\n",
    "#print(a)\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "\n",
    "principalComponents = pca.fit_transform(a)\n",
    "\n",
    "principalDf = pd.DataFrame(data = principalComponents)\n",
    "print(principalDf.shape)\n",
    "#finalDf = pd.concat([principalDf, y], axis = 1)\n",
    "#finalDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features for train:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "count_chars: 100%|██████████| 39052/39052 [00:09<00:00, 4323.13it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:06<00:00, 5994.40it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:05<00:00, 7667.39it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:01<00:00, 32674.29it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:00<00:00, 339877.12it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:06<00:00, 5800.36it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:04<00:00, 7963.75it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:01<00:00, 32945.49it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:00<00:00, 594070.61it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:07<00:00, 5549.92it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:04<00:00, 8632.10it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:01<00:00, 34764.83it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:00<00:00, 613254.41it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:06<00:00, 6188.96it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:04<00:00, 8141.06it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:01<00:00, 34240.53it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:00<00:00, 555007.25it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:06<00:00, 6324.34it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:04<00:00, 8091.41it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:01<00:00, 33223.52it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:00<00:00, 559030.58it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:01<00:00, 23916.42it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:01<00:00, 30142.55it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:00<00:00, 90537.07it/s]\n",
      "strstat: 100%|██████████| 39052/39052 [00:28<00:00, 1382.72it/s]\n",
      "str_digit_stat: 100%|██████████| 39052/39052 [00:27<00:00, 1425.88it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>ciphertext</th>\n",
       "      <th>target</th>\n",
       "      <th>nunique</th>\n",
       "      <th>len</th>\n",
       "      <th>n_l</th>\n",
       "      <th>n_n</th>\n",
       "      <th>n_s</th>\n",
       "      <th>n_ul</th>\n",
       "      <th>...</th>\n",
       "      <th>str_max</th>\n",
       "      <th>str_skew</th>\n",
       "      <th>str_kurtosis</th>\n",
       "      <th>str_digit_sum</th>\n",
       "      <th>str_digit_mean</th>\n",
       "      <th>str_digit_std</th>\n",
       "      <th>str_digit_min</th>\n",
       "      <th>str_digit_max</th>\n",
       "      <th>str_digit_skew</th>\n",
       "      <th>str_digit_kurtosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_88b9bbd73</td>\n",
       "      <td>4</td>\n",
       "      <td>ob|I\u0006\f",
       "K?zzhX*L{83B3Z,\u0006FuL*Pusm$83L\\t@r$$*38,8s...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>300</td>\n",
       "      <td>0.473333</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.356667</td>\n",
       "      <td>0.246667</td>\n",
       "      <td>...</td>\n",
       "      <td>127.0</td>\n",
       "      <td>-0.091650</td>\n",
       "      <td>-0.950683</td>\n",
       "      <td>2664.0</td>\n",
       "      <td>52.235294</td>\n",
       "      <td>3.299366</td>\n",
       "      <td>48.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>-0.001394</td>\n",
       "      <td>-1.549887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_f489bd59f</td>\n",
       "      <td>1</td>\n",
       "      <td>c1|FaAO120O'8o\u0002vfoy1W#at\u001bvGs1[1s1[1/1]O-a8o1-\u001b...</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>300</td>\n",
       "      <td>0.383333</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.376667</td>\n",
       "      <td>0.103333</td>\n",
       "      <td>...</td>\n",
       "      <td>124.0</td>\n",
       "      <td>-0.047954</td>\n",
       "      <td>-1.144330</td>\n",
       "      <td>3568.0</td>\n",
       "      <td>49.555556</td>\n",
       "      <td>2.146631</td>\n",
       "      <td>48.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>2.564841</td>\n",
       "      <td>4.938975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_f90fee9c7</td>\n",
       "      <td>2</td>\n",
       "      <td>1*e4N8$f$0ccOui\u0018hkHek\u001a$\u0010k*\u001aV*hoe\u0010V\u001a$Hj8\bV\u0003hH8...</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>300</td>\n",
       "      <td>0.430000</td>\n",
       "      <td>0.146667</td>\n",
       "      <td>0.423333</td>\n",
       "      <td>0.176667</td>\n",
       "      <td>...</td>\n",
       "      <td>127.0</td>\n",
       "      <td>-0.063138</td>\n",
       "      <td>-1.231967</td>\n",
       "      <td>2406.0</td>\n",
       "      <td>54.681818</td>\n",
       "      <td>2.274091</td>\n",
       "      <td>48.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>-1.587334</td>\n",
       "      <td>1.299930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_8303ced65</td>\n",
       "      <td>1</td>\n",
       "      <td>O8v^10\u001bO#to1'#^'^\u0002\u001b\u0002tv1^]s111t0\u001b1O\u0002taq&gt;\u001b-ata_1...</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>300</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.246667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.153333</td>\n",
       "      <td>...</td>\n",
       "      <td>125.0</td>\n",
       "      <td>-0.077411</td>\n",
       "      <td>-0.983794</td>\n",
       "      <td>3682.0</td>\n",
       "      <td>49.756757</td>\n",
       "      <td>2.276857</td>\n",
       "      <td>48.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>2.245122</td>\n",
       "      <td>3.317398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_72abc2cb7</td>\n",
       "      <td>2</td>\n",
       "      <td>e\u001aV}H\u001a}kh\u001afe4b8'S.Vc}{A\f",
       "\f",
       ".#VikV.\u001afV?{$f7\u001a$Hjb8...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>300</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.113333</td>\n",
       "      <td>0.453333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>...</td>\n",
       "      <td>126.0</td>\n",
       "      <td>-0.241807</td>\n",
       "      <td>-1.200268</td>\n",
       "      <td>1875.0</td>\n",
       "      <td>55.147059</td>\n",
       "      <td>1.477923</td>\n",
       "      <td>51.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>-1.675256</td>\n",
       "      <td>1.324605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Id  difficulty  \\\n",
       "0  ID_88b9bbd73           4   \n",
       "1  ID_f489bd59f           1   \n",
       "2  ID_f90fee9c7           2   \n",
       "3  ID_8303ced65           1   \n",
       "4  ID_72abc2cb7           2   \n",
       "\n",
       "                                          ciphertext  target  nunique  len  \\\n",
       "0  ob|I\u0006\n",
       "K?zzhX*L{83B3Z,\u0006FuL*Pusm$83L\\t@r$$*38,8s...      10        1  300   \n",
       "1  c1|FaAO120O'8o\u0002vfoy1W#at\u001bvGs1[1s1[1/1]O-a8o1-\u001b...      13        1  300   \n",
       "2  1*e4N8$f$0ccOui\u0018hkHek\u001a$\u0010k*\u001aV*hoe\u0010V\u001a$Hj8\bV\u0003hH8...      19        1  300   \n",
       "3  O8v^10\u001bO#to1'#^'^\u0002\u001b\u0002tv1^]s111t0\u001b1O\u0002taq>\u001b-ata_1...      17        1  300   \n",
       "4  e\u001aV}H\u001a}kh\u001afe4b8'S.Vc}{A\n",
       "\n",
       ".#VikV.\u001afV?{$f7\u001a$Hjb8...       0        1  300   \n",
       "\n",
       "        n_l       n_n       n_s      n_ul         ...          str_max  \\\n",
       "0  0.473333  0.170000  0.356667  0.246667         ...            127.0   \n",
       "1  0.383333  0.240000  0.376667  0.103333         ...            124.0   \n",
       "2  0.430000  0.146667  0.423333  0.176667         ...            127.0   \n",
       "3  0.420000  0.246667  0.333333  0.153333         ...            125.0   \n",
       "4  0.433333  0.113333  0.453333  0.133333         ...            126.0   \n",
       "\n",
       "   str_skew  str_kurtosis  str_digit_sum  str_digit_mean  str_digit_std  \\\n",
       "0 -0.091650     -0.950683         2664.0       52.235294       3.299366   \n",
       "1 -0.047954     -1.144330         3568.0       49.555556       2.146631   \n",
       "2 -0.063138     -1.231967         2406.0       54.681818       2.274091   \n",
       "3 -0.077411     -0.983794         3682.0       49.756757       2.276857   \n",
       "4 -0.241807     -1.200268         1875.0       55.147059       1.477923   \n",
       "\n",
       "   str_digit_min  str_digit_max  str_digit_skew  str_digit_kurtosis  \n",
       "0           48.0           57.0       -0.001394           -1.549887  \n",
       "1           48.0           56.0        2.564841            4.938975  \n",
       "2           48.0           57.0       -1.587334            1.299930  \n",
       "3           48.0           56.0        2.245122            3.317398  \n",
       "4           51.0           56.0       -1.675256            1.324605  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature extraction\n",
    "import datetime\n",
    "import gc\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.stats import skew, kurtosis\n",
    "import lightgbm as lgb\n",
    "\n",
    "import Levenshtein\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "train = pd.read_csv('20-newsgroups-ciphertext-challenge/train.csv')\n",
    "data_1 = train.query('difficulty==1')\n",
    "X = data_1.iloc[:,:-1]\n",
    "y = data_1.iloc[:,-1]\n",
    "\n",
    "'''\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(trainDF['text'])\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(trainDF['text'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "\n",
    "ensemble.RandomForestClassifier()\n",
    "\n",
    "diff1['ciphertext'] = diff1['ciphertext'].apply(lambda x: x.replace('1', ' '))\n",
    "diff2['ciphertext'] = diff2['ciphertext'].apply(lambda x: x.replace('8', ' '))\n",
    "diff3['ciphertext'] = diff3['ciphertext'].apply(lambda x: x.replace('8', ' '))\n",
    "diff4['ciphertext'] = diff4['ciphertext'].apply(lambda x: x.replace('8', ' '))\n",
    "'''\n",
    "\n",
    "def extract_features(df):\n",
    "    #df['nunique'] = df['ciphertext'].apply(lambda x: len(np.unique(x)))\n",
    "    #df['len'] = df['ciphertext'].apply(lambda x: len(x))\n",
    "\n",
    "    def count_chars(x):\n",
    "        n_l = 0 # count letters\n",
    "        n_n = 0 # count numbers\n",
    "        n_s = 0 # count symbols\n",
    "        n_ul = 0 # count upper letters\n",
    "        n_ll = 0 # count lower letters\n",
    "        for i in range(0, len(x)):\n",
    "            if x[i].isalpha():\n",
    "                n_l += 1\n",
    "                if x[i].isupper():\n",
    "                    n_ul += 1\n",
    "                elif x[i].islower():\n",
    "                    n_ll += 1\n",
    "            elif x[i].isdigit():\n",
    "                n_n += 1\n",
    "            else:\n",
    "                n_s += 1\n",
    "\n",
    "        return pd.Series([n_l, n_n, n_s, n_ul, n_ll])\n",
    "\n",
    "    cols = ['n_l', 'n_n', 'n_s', 'n_ul', 'n_ll']\n",
    "    for c in cols:\n",
    "        df[c] = 0\n",
    "    tqdm.pandas(desc='count_chars')\n",
    "    df[cols] = df['ciphertext'].progress_apply(lambda x: count_chars(x))\n",
    "    for c in cols:\n",
    "        df[c] /= df['len']\n",
    "\n",
    "    tqdm.pandas(desc='distances')\n",
    "    df['Levenshtein_distance'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.distance(x, x[::-1]))\n",
    "    df['Levenshtein_ratio'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.ratio(x, x[::-1]))\n",
    "    df['Levenshtein_jaro'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.jaro(x, x[::-1]))\n",
    "    df['Levenshtein_hamming'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.hamming(x, x[::-1]))\n",
    "\n",
    "    for m in range(1, 5):\n",
    "        df['Levenshtein_distance_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.distance(x[:-m], x[m:]))\n",
    "        df['Levenshtein_ratio_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.ratio(x[:-m], x[m:]))\n",
    "        df['Levenshtein_jaro_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.jaro(x[:-m], x[m:]))\n",
    "        df['Levenshtein_hamming_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.hamming(x[:-m], x[m:]))\n",
    "    \n",
    "    df['Levenshtein_distance_h'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.distance(x[:len(x)//2], x[len(x)//2:]))\n",
    "    df['Levenshtein_ratio_h'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.ratio(x[:len(x)//2], x[len(x)//2:]))\n",
    "    df['Levenshtein_jaro_h'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.jaro(x[:len(x)//2], x[len(x)//2:]))\n",
    "    \n",
    "    # All symbols stats\n",
    "    def strstat(x):\n",
    "        r = np.array([ord(c) for c in x])\n",
    "        return pd.Series([\n",
    "            np.sum(r), \n",
    "            np.mean(r), \n",
    "            np.std(r), \n",
    "            np.min(r), \n",
    "            np.max(r),\n",
    "            skew(r), \n",
    "            kurtosis(r),\n",
    "            ])\n",
    "    cols = ['str_sum', 'str_mean', 'str_std', 'str_min', 'str_max', 'str_skew', 'str_kurtosis']\n",
    "    for c in cols:\n",
    "        df[c] = 0\n",
    "    tqdm.pandas(desc='strstat')\n",
    "    df[cols] = df['ciphertext'].progress_apply(lambda x: strstat(x))\n",
    "    \n",
    "    # Digit stats\n",
    "    def str_digit_stat(x):\n",
    "        r = np.array([ord(c) for c in x if c.isdigit()])\n",
    "        if len(r) == 0:\n",
    "            r = np.array([0])\n",
    "        return pd.Series([\n",
    "            np.sum(r), \n",
    "            np.mean(r), \n",
    "            np.std(r), \n",
    "            np.min(r), \n",
    "            np.max(r),\n",
    "            skew(r), \n",
    "            kurtosis(r),\n",
    "            ])\n",
    "    cols = ['str_digit_sum', 'str_digit_mean', 'str_digit_std', 'str_digit_min', \n",
    "        'str_digit_max', 'str_digit_skew', 'str_digit_kurtosis']\n",
    "    for c in cols:\n",
    "        df[c] = 0\n",
    "    tqdm.pandas(desc='str_digit_stat')\n",
    "    df[cols] = df['ciphertext'].progress_apply(lambda x: str_digit_stat(x))\n",
    "\n",
    "print('Extracting features for train:')\n",
    "extract_features(train)\n",
    "train.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
