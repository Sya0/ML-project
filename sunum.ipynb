{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import gc\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import timeit\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.metrics import log_loss,confusion_matrix,roc_curve,auc\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20 newsgroups challenge problemi, machine learning açısından yıllarca kullanılan bilindik bir problemdir. Bu problemde 20 adet konu başlığı bulunmaktadır. Bu farklı gazete yazılarının hangi konu başlığına ait olduğunu bulmaya çalışılır. 20 newsgroups ciphertext challenge da ise başlıkta da anlaşılacağı üzere gazete yazıları şifrenlenmiştir. 4 farklı zorluk derecesine göre şifrelenen gazete yazıları sırasıyla 1,2,3,4 zorluk derecesine sahiptir. 1 zorluk derecesindeki yazılar 1 adet şifreleme geçirmiş, 2 zorluk derecesine sahip yazılar ise art arda 2 adet şifreleme geçirmiştir. Diğerleride aynı şekilde şifrelenmişlerdir.\n",
    "\n",
    "Machine learning alanında dil ile ilgili olan problemlere NLP (Neuro-linguistic programming) denir. Bu tür problemlerde yapılan başlıca adımlar vardır. Bunlardan bazıları şunlardır;\n",
    "\n",
    "1-)Yazıyı Temizlemek: \n",
    "- Alakasız karakterleri yazıdan silmek. Bunlara bazı gereksiz sayılar, noktalama işaretleri veya İngilizcedeki \"am\", \"is\", \"are\" gibi kelimeler dahildir.\n",
    "- Yazılardaki bütün harfleri küçüük harfe çevirmek çünkü “hello”, “Hello”, ve “HELLO” gibi kelimelerin hepsi aynı şeyi ifade ediyor.\n",
    "\n",
    "2-)Modellemek için kelimeleri birbirinden ayırmak\n",
    "- Modellemede kullanmak için yazı içindeki her kelimeyi ayrı bir eleman gibi düşünmek. Buna ingilizcede \"Tokenize\" deniyor. \n",
    "\n",
    "3-)Vectore çevirmek\n",
    "- Machine learning de kullanılan modeller kelime veya karakter bazlı olmadığı için yazıları sayıya çevirmek gerekiyor. Bunu yapmak içinde kelimeleri veya belli sayıda karakterleri çeşitli fonksiyonlardan geçirerek onların vektor karşılıklarıyla işlem yapılmalıdır. \n",
    "\n",
    "4-)Özellik çoğaltma\n",
    "- NLP problemlerinde genelde yazının yanında herhangi bir özellik verilmez. Vectore çevirme aslında bir nevi yazının özelliğini ortaya çıkartma olarak algılanabilir. Bunların yanında yazıların özelliklerini çıkartmak için python da kullanılan bazı yöntemler vardır. Bu yöntemler fuzzy,Levenshtein gibi yardımcı araçlarla ortaya çıkartılabilir.\n",
    "\n",
    "4-)Model bulmak\n",
    "- NLP için sıklıkla kullanılan bazı modelleme yöntemleri vardır. Bunlardan bazıları Naive-Bayes, SVM, Logistic Regression, Ensemble gibi modellemelerdir. Bunların her biri denenip hangisinin daha iyi sonuç veridiği bulunmalıdır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adım 1:\n",
    "\n",
    "\"c1|FaAO120O'8o\u0002vfoy1W#at\u001bvGs1[1s1[1/1]O-a8o1\" yazısı zorluk derecesi 1 olan şifrenlenmiş bir yazı örneğidir. Burada ilk olarak herhangi bir temizleme işlemi yapmadan yazılar vectorlere çevrilerek Naive-Bayes, SVM, Logistic Regression, RandomForest ve XGBClassifier modelleri denenerek sonuçlar analiz edilmiştir. Burada verinin büyüklüğü nedeniyle sadece zorluk derecesi 1 olan verilerle çalışılmıştır. \n",
    "\n",
    "Vectore çevirmek için ilk başta CountVectorizer fonksiyonu kullanılmıştır. Şifrelenmiş verilerde büyük küçük harf önemli olduğu için büyük harfler küçük harfe çevrilmemiştir. Tokinizer olarak ngram kullanılmıştır. Fonksiyon çıkışında her kelime ayrılıp bir vector olarak ifade edilmiştir. Vectorizer da en iyi parametreleri bulmak için GridSearch yöntemi kullanılmıştır. Burada analyzer parametresi olarak ('word','char'), ngram parametresi olarak birçok değer denemiştir. Bulunan en iyi sonuç ile yazılar vectorlere çevrilmiştir. Daha sonra modellere uygulanarak sırasıyla sonuçlar gözlemlenmiştir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Modelling\n",
    "df = pd.read_csv('20-newsgroups-ciphertext-challenge/train.csv')\n",
    "data_1 = df.query('difficulty==1')\n",
    "X = data_1.iloc[:,-2]\n",
    "y = data_1.iloc[:,-1]\n",
    "\n",
    "parameters = {\n",
    "    #'clf__alpha': (1.0000000000000001e-05, 9.9999999999999995e-07),\n",
    "    #'clf__max_iter': (10, 50, 80),\n",
    "    #'clf__penalty': ('l2', 'elasticnet'),\n",
    "    #'clf__C': (1e5,2.65),\n",
    "    #'tfidf__use_idf': (True, False),\n",
    "    #'tfidf__sublinear_tf': (True, False),\n",
    "    #'tfidf__lowercase': (True,False),\n",
    "    #'tfidf__strip_accents': ('ascii','unicode'),\n",
    "    #'tfidf__analyzer': ('word','char'),\n",
    "    #'tfidf__ngram_range': ((1,4),(1,5)),\n",
    "    #'tfidf__max_features': (7500,5000),\n",
    "    #'svd__n_components': (500,1000),\n",
    "    #'vect__ngram_range': ((1, 6), (1, 7)),\n",
    "    #'vect__analyzer': ('word','char'),\n",
    "    #'vect__min_df': (0.001,0.01),\n",
    "}\n",
    "\n",
    "# build TFIDF Vectorizer\n",
    "tokens= ((u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b')) # makes sure it matches a word but contains at least one letter\n",
    "\n",
    "nb = Pipeline([('vect', CountVectorizer(analyzer = 'char',ngram_range=(1, 6))),\n",
    "               #('tfidf', TfidfVectorizer(strip_accents='ascii',analyzer='char',token_pattern=tokens,\n",
    "               #                          ngram_range=(1,4),lowercase=False,dtype=np.uint32,max_features=7500)),\n",
    "               #('svd', TruncatedSVD(algorithm='arpack')),\n",
    "               ('clf', MultinomialNB(alpha=1.0e-10)),])\n",
    "#grid_search = GridSearchCV(nb, parameters, cv=5,n_jobs=-1, verbose=1)\n",
    "\n",
    "lr = Pipeline([#('vect', CountVectorizer(analyzer = 'word',ngram_range=(1, 2))),\n",
    "               ('tfidf', TfidfVectorizer(strip_accents='ascii',analyzer='word',token_pattern=tokens,\n",
    "                                         ngram_range=(1,2),lowercase=False,dtype=np.float32,max_features=7500)),\n",
    "               ('clf', LogisticRegression(solver='saga',max_iter=100,class_weight='balanced',C=1e5,\n",
    "                                          penalty='l2',n_jobs=-1,multi_class='auto')),])\n",
    "grid_search = GridSearchCV(lr, parameters, cv=5,n_jobs=-1, verbose=1)\n",
    "\n",
    "svm = Pipeline([#('vect', CountVectorizer(analyzer = 'char',ngram_range=(1, 6))),\n",
    "                ('tfidf', TfidfVectorizer(strip_accents='ascii',analyzer='char',token_pattern=tokens,\n",
    "                                         ngram_range=(1,4),lowercase=False,dtype=np.float32,max_features=7500)),\n",
    "                ('clf', SGDClassifier(loss='hinge',penalty='l2',alpha=1e-5,random_state=0,max_iter=1000, \n",
    "                                     tol=1e-3,n_jobs=-1)),])\n",
    "#grid_search = GridSearchCV(svm, parameters, cv=5,n_jobs=-1, verbose=1)\n",
    "\n",
    "rf = Pipeline([#('vect', CountVectorizer(analyzer = 'char',ngram_range=(1, 6))),\n",
    "               ('tfidf', TfidfVectorizer(strip_accents='ascii',analyzer='char',token_pattern=tokens,\n",
    "                                         ngram_range=(1,4),lowercase=False,dtype=np.float32,max_features=7500)), \n",
    "               ('clf', RandomForestClassifier(n_estimators=500,max_features='log2',min_samples_split=4)),])\n",
    "#grid_search = GridSearchCV(rf, parameters, cv=5,n_jobs=-1, verbose=1)\n",
    "\n",
    "xgb = Pipeline([('vect', CountVectorizer(analyzer = 'char',ngram_range=(1, 6))),\n",
    "                #('tfidf', TfidfVectorizer(tokenizer=tokens, min_df=0.001, max_df=0.3)),\n",
    "                ('svd', TruncatedSVD(algorithm='randomized', n_components=300)),\n",
    "                ('clf', XGBClassifier(max_depth=3,n_estimators=1000,learning_rate=0.075,\n",
    "                                      colsample_bytree=0.7,subsample=0.8)),])\n",
    "#grid_search = GridSearchCV(xgb, parameters, cv=5,n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 -)Naive-Bayes train score= 0.9938674321503131\n",
    "1 -)Naive-Bayes test score= 0.6046753246753247\n",
    "2 -)Naive-Bayes train score= 0.9937426671881111\n",
    "2 -)Naive-Bayes test score= 0.6235662148070907\n",
    "3 -)Naive-Bayes train score= 0.9934827945776851\n",
    "3 -)Naive-Bayes test score= 0.6061554512258738\n",
    "4 -)Naive-Bayes train score= 0.9930926625830835\n",
    "4 -)Naive-Bayes test score= 0.6132567849686847\n",
    "5 -)Naive-Bayes train score= 0.9936164669098488\n",
    "5 -)Naive-Bayes test score= 0.6121275483533717\n",
    "Elapsed time 114.89954035500705\n",
    "\n",
    "1 -)Naive-Bayes train score= 0.6860647181628392\n",
    "1 -)Naive-Bayes test score= 0.41818181818181815\n",
    "2 -)Naive-Bayes train score= 0.6782688045887107\n",
    "2 -)Naive-Bayes test score= 0.4191866527632951\n",
    "3 -)Naive-Bayes train score= 0.6904327424400417\n",
    "3 -)Naive-Bayes test score= 0.41366718831507565\n",
    "4 -)Naive-Bayes train score= 0.6881271992701681\n",
    "4 -)Naive-Bayes test score= 0.43893528183716074\n",
    "5 -)Naive-Bayes train score= 0.6827774882751433\n",
    "5 -)Naive-Bayes test score= 0.42812336644014637\n",
    "Elapsed time 58.729096541996114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-34815984cee3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mskf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Naive-Bayes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mnb_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"-)Naive-Bayes train score=\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "# trigger timer\n",
    "start_time = timeit.default_timer()\n",
    "i=1\n",
    "# create and train model\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "for train, test in skf.split(X, y):\n",
    "    # Naive-Bayes\n",
    "    nb_ext.fit(X.iloc[train,:], y.iloc[train])\n",
    "    y_pred = nb_ext.predict(X.iloc[test])\n",
    "    print(i,\"-)Naive-Bayes train score=\", nb_ext.score(X.iloc[train],y.iloc[train]))\n",
    "    print(i,\"-)Naive-Bayes test score=\", accuracy_score(y_pred, y.iloc[test]))  \n",
    "    i=i+1\n",
    "\n",
    "#print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "#print(\"Best parameters set:\")\n",
    "#best_parameters = grid_search.best_estimator_.get_params()\n",
    "#for param_name in sorted(parameters.keys()):\n",
    "#    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "# calculate time interval\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(\"Elapsed time\", elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 -)Logistic Regression Classifier train score= 0.9973387601753287\n",
    "1 -)Logistic Regression Classifier test score= 0.42018119337706966\n",
    "2 -)Logistic Regression Classifier train score= 0.9976533166458073\n",
    "2 -)Logistic Regression Classifier test score= 0.4350954019393181\n",
    "3 -)Logistic Regression Classifier train score= 0.9970303219756174\n",
    "3 -)Logistic Regression Classifier test score= 0.4261986837981824\n",
    "Elapsed time 65.17152920400258\n",
    "\n",
    "1 -)Logistic Regression Classifier train score= 0.993894802755166\n",
    "1 -)Logistic Regression Classifier test score= 0.44204935957513275\n",
    "2 -)Logistic Regression Classifier train score= 0.9937421777221527\n",
    "2 -)Logistic Regression Classifier test score= 0.44541757898029405\n",
    "3 -)Logistic Regression Classifier train score= 0.9931228508909034\n",
    "3 -)Logistic Regression Classifier test score= 0.43685365089313694\n",
    "Elapsed time 32.350123803000315"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trigger timer\n",
    "start_time = timeit.default_timer()\n",
    "i=1\n",
    "# create and train model\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "for train, test in skf.split(X, y):\n",
    "    # Logistic Regression Classifier\n",
    "    grid_search.fit(X.iloc[train], y.iloc[train])\n",
    "    y_pred = grid_search.predict(X.iloc[test])\n",
    "    print(i,\"-)Logistic Regression Classifier train score=\", grid_search.score(X.iloc[train],y.iloc[train]))\n",
    "    print(i,\"-)Logistic Regression Classifier test score=\", accuracy_score(y_pred, y.iloc[test]))\n",
    "    i=i+1\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "# calculate time interval\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(\"Elapsed time\", elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 -)SVM train score= 0.9616468378209142\n",
    "1 -)SVM test score= 0.5029678225554515\n",
    "2 -)SVM train score= 0.9586983729662077\n",
    "2 -)SVM test score= 0.5176728182671254\n",
    "3 -)SVM train score= 0.9477961863082213\n",
    "3 -)SVM test score= 0.4992165465371357\n",
    "Elapsed time 141.1674568860035\n",
    "\n",
    "1 -)SVM train score= 0.9575767063243582\n",
    "1 -)SVM test score= 0.5301468291159013\n",
    "2 -)SVM train score= 0.949468085106383\n",
    "2 -)SVM test score= 0.5317485142320926\n",
    "3 -)SVM train score= 0.9554548296342608\n",
    "3 -)SVM test score= 0.5352554058288937\n",
    "Elapsed time 43.7149073959954"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trigger timer\n",
    "start_time = timeit.default_timer()\n",
    "i=1\n",
    "# create and train model\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "for train, test in skf.split(X, y):\n",
    "    # SGDClassifier\n",
    "    svm.fit(X.iloc[train], y.iloc[train])\n",
    "    y_pred = svm.predict(X.iloc[test])\n",
    "    print(i,\"-)SVM train score=\", svm.score(X.iloc[train],y.iloc[train]))\n",
    "    print(i,\"-)SVM test score=\", accuracy_score(y_pred, y.iloc[test]))\n",
    "    i=i+1\n",
    "\n",
    "#print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "#print(\"Best parameters set:\")\n",
    "#best_parameters = grid_search.best_estimator_.get_params()\n",
    "#for param_name in sorted(parameters.keys()):\n",
    "#    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "# calculate time interval\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(\"Elapsed time\", elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 -)Random Forest train score= 0.9993738259236068\n",
    "1 -)Random Forest test score= 0.4957825679475164\n",
    "2 -)Random Forest train score= 0.9992177722152691\n",
    "2 -)Random Forest test score= 0.5032843290584923\n",
    "3 -)Random Forest train score= 0.9990622069396686\n",
    "3 -)Random Forest test score= 0.5155123785647132\n",
    "Elapsed time 514.6145224819993\n",
    "\n",
    "1 -)Random Forest train score= 0.9996869129618033\n",
    "1 -)Random Forest test score= 0.4126835363948766\n",
    "2 -)Random Forest train score= 0.9998435544430538\n",
    "2 -)Random Forest test score= 0.4119487019080388\n",
    "3 -)Random Forest train score= 0.9996874023132228\n",
    "3 -)Random Forest test score= 0.4202444374804137\n",
    "Elapsed time 131.42851219599834"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trigger timer\n",
    "start_time = timeit.default_timer()\n",
    "i=1\n",
    "# create and train model\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "for train, test in skf.split(X, y):\n",
    "    # Random Forest\n",
    "    rf.fit(X.iloc[train], y.iloc[train])\n",
    "    y_pred = rf.predict(X.iloc[test])\n",
    "    print(i,\"-)Random Forest train score=\", rf.score(X.iloc[train],y.iloc[train]))\n",
    "    print(i,\"-)Random Forest test score=\", accuracy_score(y_pred, y.iloc[test]))\n",
    "    i=i+1\n",
    "\n",
    "#print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "#print(\"Best parameters set:\")\n",
    "#best_parameters = grid_search.best_estimator_.get_params()\n",
    "#for param_name in sorted(parameters.keys()):\n",
    "#    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "# calculate time interval\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(\"Elapsed time\", elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trigger timer\n",
    "start_time = timeit.default_timer()\n",
    "i=1\n",
    "# create and train model\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "for train, test in skf.split(X, y):\n",
    "    # XGBClassifier\n",
    "    xgb.fit(X.iloc[train], y.iloc[train])\n",
    "    y_pred = xgb.predict(X.iloc[test])\n",
    "    print(i,\"-)XGBClassifier train score=\", xgb.score(X.iloc[train],y.iloc[train]))\n",
    "    print(i,\"-)XGBClassifier test score=\", accuracy_score(y_pred, y.iloc[test]))\n",
    "    i=i+1\n",
    "\n",
    "#print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "#print(\"Best parameters set:\")\n",
    "#best_parameters = grid_search.best_estimator_.get_params()\n",
    "#for param_name in sorted(parameters.keys()):\n",
    "#    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "# calculate time interval\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(\"Elapsed time\", elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    10024\n",
      "4     9970\n",
      "1     9589\n",
      "3     9469\n",
      "Name: difficulty, dtype: int64\n",
      "target       0    1    2    3    4    5    6    7    8    9    10   11   12  \\\n",
      "difficulty                                                                    \n",
      "1           420  465  360  346  320  466  361  331  380  486  540  563  344   \n",
      "2           455  528  652  391  315  550  310  405  387  301  580  651  396   \n",
      "3           394  567  695  386  313  524  293  422  331  460  479  534  338   \n",
      "4           465  366  940  382  326  653  272  437  386  413  437  622  429   \n",
      "\n",
      "target       13   14   15   16   17   18   19  \n",
      "difficulty                                     \n",
      "1           482  576  765  471  834  675  404  \n",
      "2           500  566  692  536  912  547  350  \n",
      "3           593  470  513  506  714  553  384  \n",
      "4           599  463  582  589  693  539  377  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "c     53752\n",
       "1    505426\n",
       "|      7490\n",
       "F      6134\n",
       "a    121319\n",
       "A     58440\n",
       "O    134813\n",
       "2      8502\n",
       "0     75960\n",
       "'     34490\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataframe for only difficulty=1\n",
    "data1 = df.query('difficulty==1')\n",
    "# how many elements do have diffuculty class\n",
    "print(df['difficulty'].value_counts()) \n",
    "# count repeated chars in ciphertext class\n",
    "alp = pd.Series(Counter(''.join(data1['ciphertext'])))  \n",
    "print(pd.crosstab(df['difficulty'], df['target']))\n",
    "alp.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adım 2:\n",
    "\n",
    "Yazılar vectorel hale çevrildikten sonra yazı üzerinde bazı temizlemeler yapılmıştır. Buradaki amaç mümkün olan en az şekilde kripto analiz yapılarak en iyi sonucu elde etmektir. Yukarıdaki çıktıda görüldüğü üzere yazı üzerinde bazı frekans analizi yapılmıştır. Bu çıktı sonucunda '1' karakteri zorluk derecesi 1 için ' ' (boşluk) karakterine tekabül ettiği anlaşılır. Bu yüzden yazı içinde kelimeleri ayırmak için yazı içindeki '1' karakteri yerine ' ' karakteri yazılıp tokenize işleminin daha iyi yapılması sağlanır.\n",
    "\n",
    "Bununla birlikte her ne kadar şifrelenmiş yazı üzerinde temizleme işlemi zor olsada birkaç yöntem kullanılabilir. Bunlardan biri 3 karakter ve daha az uzunluğa sahip kelimeler silinebilir. Bunlar İngilizce de daha çok \"am\", \"is\", \"are\", \"the\" gibi çok anlam ifade etmeyen kelimelerdir. Bu yüzden bunları silmek hem yazının boyutunu düşürecektir hem de sınfılandırma kesinliğinin artacağı düşünülmektedir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('20-newsgroups-ciphertext-challenge/train.csv')\n",
    "data_1 = df.query('difficulty==1')\n",
    "X = data_1.iloc[:,-2]\n",
    "y = data_1.iloc[:,-1]\n",
    "\n",
    "'''\n",
    "diff1['ciphertext'] = diff1['ciphertext'].apply(lambda x: x.replace('1', ' '))\n",
    "diff2['ciphertext'] = diff2['ciphertext'].apply(lambda x: x.replace('8', ' '))\n",
    "diff3['ciphertext'] = diff3['ciphertext'].apply(lambda x: x.replace('8', ' '))\n",
    "diff4['ciphertext'] = diff4['ciphertext'].apply(lambda x: x.replace('8', ' '))\n",
    "'''\n",
    "\n",
    "def tokenize(text): \n",
    "    return text.split(\"1\")\n",
    "\n",
    "def trimm(text):\n",
    "    return ' '.join([i for i in text if len(i) > 3])\n",
    "\n",
    "token_data = [tokenize(i) for i in X]\n",
    "X = [trimm(i) for i in token_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adım 3:\n",
    "\n",
    "Yazı üzerinde bazı temizlemeler ve analizler yapıldıktan sonra özellik genişletme uygulanmıştır. Burada 2 adet tfidf fonksiyonu kullanılmıştır; bir tanesi karakter, diğeri kelime özelliklerini ortaya çıkaran vektore çevirmek fonksiyonlarıdır. Bunlar tek bir dizide birleştirilerek yukarıdaki çeşitli sınıflandırma yöntemleriyle modellenmiştir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-38c174748f38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# RandomForest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mrf_ext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'log2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_samples_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mrf_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RandomForest train score=\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrf_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    331\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 333\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    799\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    802\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    364\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# extract feature by TFIDF Vectorizer\n",
    "tokens= ((u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b')) # makes sure it matches a word but contains at least one letter\n",
    "\n",
    "word_vectorizer = TfidfVectorizer(strip_accents='ascii',lowercase=False,analyzer='word',token_pattern=tokens,\n",
    "                                  ngram_range=(1,2),dtype=np.float32,max_features=7500)\n",
    "\n",
    "char_vectorizer = TfidfVectorizer(strip_accents='ascii',lowercase=False,analyzer='char',token_pattern=tokens,\n",
    "                                  ngram_range=(2, 4),dtype=np.float32,max_features=12000)\n",
    "\n",
    "word_vectorizer.fit(X)\n",
    "char_vectorizer.fit(X)\n",
    "\n",
    "train_word_features = word_vectorizer.transform(X)\n",
    "train_char_features = char_vectorizer.transform(X)\n",
    "\n",
    "#lda = TruncatedSVD(algorithm='randomized', n_components=300)\n",
    "a = hstack([train_char_features,train_word_features])\n",
    "#b = lda.fit_transform(a)\n",
    "X = pd.DataFrame(a.toarray())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "# Naive-Bayes\n",
    "#nb_ext = MultinomialNB(alpha=1.0e-10)\n",
    "#nb_ext.fit(X,y)\n",
    "#y_pred = nb_ext.predict(X_test)\n",
    "#print(\"Naive-Bayes train score=\", nb_ext.score(X_train,y_train))\n",
    "#print(\"Naive-Bayes test score=\", accuracy_score(y_pred,y_test))\n",
    "\n",
    "# LogisticRegression\n",
    "#lr_ext = LogisticRegression(solver='saga',max_iter=100,class_weight='balanced',C=10,penalty='l2',\n",
    "#                            n_jobs=-1,multi_class='auto')\n",
    "#lr_ext.fit(X,y)\n",
    "#y_pred = lr_ext.predict(X_test)\n",
    "#print(\"LogisticRegression train score=\", lr_ext.score(X_train,y_train))\n",
    "#print(\"LogisticRegression test score=\", accuracy_score(y_pred,y_test))\n",
    "\n",
    "# SVM\n",
    "#svm_ext = SGDClassifier(loss='hinge',penalty='l2',alpha=1e-5,random_state=0,max_iter=1000,tol=1e-3,n_jobs=-1)\n",
    "#svm_ext.fit(X,y)\n",
    "#y_pred = svm_ext.predict(X_test)\n",
    "#print(\"SVM train score=\", svm_ext.score(X_train,y_train))\n",
    "#print(\"SVM test score=\", accuracy_score(y_pred,y_test))\n",
    "\n",
    "# RandomForest\n",
    "rf_ext = RandomForestClassifier(n_estimators=5000,max_features='log2',min_samples_split=4)\n",
    "rf_ext.fit(X,y)\n",
    "y_pred = rf_ext.predict(X_test)\n",
    "print(\"RandomForest train score=\", rf_ext.score(X_train,y_train))\n",
    "print(\"RandomForest test score=\", accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Çıkan özelliklerin birleştirilmesi sonrasında matris boyutları (9589, 19500) olmuştur. Burada özellik seçmesi yapılarak boyut azaltılmaya çalışılmıştır fakat hatalardan dolayı yapılamamıştır. Burada modellemeler çok uzun sürdüğü için cross validation yapılmamıştır. Çıkan sonuçlar gayet tatmin edicidir. Bu sonuçlar içinde en iyi sonuç veren SVM yöntemidir. Doğruluk oranını biraz daha arttırmak için bu matris üzerinden farklı özellik arttırma yöntemleri de kullanılmıştır. \n",
    "\n",
    "LogisticRegression train score=0.944265723453451\n",
    "LogisticRegression test score=0.9169036334913112\n",
    "\n",
    "SVM train score= 0.9950186799501868\n",
    "SVM test score= 0.9936808846761453\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i=1\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "for train, test in skf.split(X, y):\n",
    "    # Naive-Bayes\n",
    "    nb_ext.fit(X.iloc[train,:],y.iloc[train])\n",
    "    y_pred = nb_ext.predict(X.iloc[test,:])\n",
    "    print(i,\"-)Naive-Bayes train score=\", nb_ext.score(X.iloc[train,:],y.iloc[train]))\n",
    "    print(i,\"-)Naive-Bayes test score=\", accuracy_score(y_pred,y.iloc[test]))  \n",
    "    \n",
    "    # LogisticRegression\n",
    "    lr_ext = LogisticRegression(solver='saga',max_iter=1000,class_weight='balanced',C=1e5,penalty='l2',\n",
    "                                n_jobs=-1,multi_class='auto')\n",
    "    lr_ext.fit(X.iloc[train,:], y.iloc[train])\n",
    "    y_pred = lr_ext.predict(X.iloc[test])\n",
    "    print(i,\"-)LogisticRegression train score=\", lr_ext.score(X.iloc[train],y.iloc[train]))\n",
    "    print(i,\"-)LogisticRegression test score=\", accuracy_score(y_pred,y.iloc[test]))\n",
    "\n",
    "    # SVM\n",
    "    svm_ext = SGDClassifier(loss='hinge',penalty='l2',alpha=1e-5,random_state=0,max_iter=1000,tol=1e-3,n_jobs=-1)\n",
    "    svm_ext.fit(X.iloc[train,:], y.iloc[train])\n",
    "    y_pred = svm_ext.predict(X.iloc[test])\n",
    "    print(i,\"-)SVM train score=\", svm_ext.score(X.iloc[train],y.iloc[train]))\n",
    "    print(i,\"-)SVM test score=\", accuracy_score(y_pred,y.iloc[test]))\n",
    "\n",
    "    # RandomForest\n",
    "    rf_ext = RandomForestClassifier(n_estimators=500,max_features='log2',min_samples_split=4)\n",
    "    rf_ext.fit(X.iloc[train,:], y.iloc[train])\n",
    "    y_pred = rf_ext.predict(X.iloc[test])\n",
    "    print(i,\"-)RandomForest train score=\", rf_ext.score(X.iloc[train],y.iloc[train]))\n",
    "    print(i,\"-)RandomForest test score=\", accuracy_score(y_pred,y.iloc[test]))\n",
    "    \n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Özellikleri çoğaltmak için kullanılan bir yöntemde cosine metric sistemi kullanılarak uygulanan benzerlik yöntemi. Burada yukarıda çıkan özellikler arasındaki benzerlikler bulunarak daha kesin bir sonuç elde edilmeye çalışılmıştır. Burada çıkan matris sonucu çok büyük olduğu için PCA yöntemi ile boyutu küçültülmeye çalışılmıştır. Çıkan sonuç modeller üzerinde denenmiştir fakat çok kötü sonuçlar alındığından bu yöntem çıkartılmıştır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cosine similarity and PCA \n",
    "a=cosine_similarity(train_word_features, train_word_features)\n",
    "\n",
    "pca = PCA(n_components=500)\n",
    "principalComponents = pca.fit_transform(a)\n",
    "\n",
    "principalDf = pd.DataFrame(data = principalComponents)\n",
    "print(principalDf.shape)\n",
    "#finalDf = pd.concat([principalDf, y], axis = 1)\n",
    "#finalDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bir başka özellik çoğaltma olarak kelimelerin ve harflerin analizi yapılmıştır. Burada python da hazır bir fonksiyon olan ve kelimeler arasındaki benzerliği ifade eden Levenshtein fonksiyonu kullanılmıştır. Bu özellik genişletme metodunun çoğu https://www.kaggle.com/adnanrana/classification-using-features linkinden alınmıştır. Buradaki özellikler de kullanılarak tahmin etme yüzdesi yükseltilmeğe çalışılmıştır fakat burada da istenilen sonuç alınamamıştır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features for train:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uzun/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'ciphertext'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   3123\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3124\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mlibindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3125\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.get_value_box\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.get_value_box\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object cannot be interpreted as an integer",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-3c0d78cadc99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Extracting features for train:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-3c0d78cadc99>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'count_chars'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ciphertext'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcount_chars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'len'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   3130\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3131\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3132\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3133\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3134\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   3116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3117\u001b[0m             return self._engine.get_value(s, k,\n\u001b[0;32m-> 3118\u001b[0;31m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[1;32m   3119\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3120\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minferred_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'integer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'boolean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ciphertext'"
     ]
    }
   ],
   "source": [
    "# Feature extraction\n",
    "def extract_features(df):\n",
    "    #df['nunique'] = df['ciphertext'].apply(lambda x: len(np.unique(x)))\n",
    "    #df['len'] = df['ciphertext'].apply(lambda x: len(x))\n",
    "\n",
    "    def count_chars(x):\n",
    "        n_l = 0 # count letters\n",
    "        n_n = 0 # count numbers\n",
    "        n_s = 0 # count symbols\n",
    "        n_ul = 0 # count upper letters\n",
    "        n_ll = 0 # count lower letters\n",
    "        for i in range(0, len(x)):\n",
    "            if x[i].isalpha():\n",
    "                n_l += 1\n",
    "                if x[i].isupper():\n",
    "                    n_ul += 1\n",
    "                elif x[i].islower():\n",
    "                    n_ll += 1\n",
    "            elif x[i].isdigit():\n",
    "                n_n += 1\n",
    "            else:\n",
    "                n_s += 1\n",
    "\n",
    "        return pd.Series([n_l, n_n, n_s, n_ul, n_ll])\n",
    "\n",
    "    cols = ['n_l', 'n_n', 'n_s', 'n_ul', 'n_ll']\n",
    "    for c in cols:\n",
    "        df[c] = 0\n",
    "    tqdm.pandas(desc='count_chars')\n",
    "    df[cols] = df['ciphertext'].progress_apply(lambda x: count_chars(x))\n",
    "    for c in cols:\n",
    "        df[c] /= df['len']\n",
    "\n",
    "    tqdm.pandas(desc='distances')\n",
    "    df['Levenshtein_distance'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.distance(x, x[::-1]))\n",
    "    df['Levenshtein_ratio'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.ratio(x, x[::-1]))\n",
    "    df['Levenshtein_jaro'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.jaro(x, x[::-1]))\n",
    "    df['Levenshtein_hamming'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.hamming(x, x[::-1]))\n",
    "\n",
    "    for m in range(1, 5):\n",
    "        df['Levenshtein_distance_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.distance(x[:-m], x[m:]))\n",
    "        df['Levenshtein_ratio_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.ratio(x[:-m], x[m:]))\n",
    "        df['Levenshtein_jaro_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.jaro(x[:-m], x[m:]))\n",
    "        df['Levenshtein_hamming_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.hamming(x[:-m], x[m:]))\n",
    "    \n",
    "    df['Levenshtein_distance_h'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.distance(x[:len(x)//2], x[len(x)//2:]))\n",
    "    df['Levenshtein_ratio_h'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.ratio(x[:len(x)//2], x[len(x)//2:]))\n",
    "    df['Levenshtein_jaro_h'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.jaro(x[:len(x)//2], x[len(x)//2:]))\n",
    "    \n",
    "    # All symbols stats\n",
    "    def strstat(x):\n",
    "        r = np.array([ord(c) for c in x])\n",
    "        return pd.Series([\n",
    "            np.sum(r), \n",
    "            np.mean(r), \n",
    "            np.std(r), \n",
    "            np.min(r), \n",
    "            np.max(r),\n",
    "            skew(r), \n",
    "            kurtosis(r),\n",
    "            ])\n",
    "    cols = ['str_sum', 'str_mean', 'str_std', 'str_min', 'str_max', 'str_skew', 'str_kurtosis']\n",
    "    for c in cols:\n",
    "        df[c] = 0\n",
    "    tqdm.pandas(desc='strstat')\n",
    "    df[cols] = df['ciphertext'].progress_apply(lambda x: strstat(x))\n",
    "    \n",
    "    # Digit stats\n",
    "    def str_digit_stat(x):\n",
    "        r = np.array([ord(c) for c in x if c.isdigit()])\n",
    "        if len(r) == 0:\n",
    "            r = np.array([0])\n",
    "        return pd.Series([\n",
    "            np.sum(r), \n",
    "            np.mean(r), \n",
    "            np.std(r), \n",
    "            np.min(r), \n",
    "            np.max(r),\n",
    "            skew(r), \n",
    "            kurtosis(r),\n",
    "            ])\n",
    "    cols = ['str_digit_sum', 'str_digit_mean', 'str_digit_std', 'str_digit_min', \n",
    "        'str_digit_max', 'str_digit_skew', 'str_digit_kurtosis']\n",
    "    for c in cols:\n",
    "        df[c] = 0\n",
    "    tqdm.pandas(desc='str_digit_stat')\n",
    "    df[cols] = df['ciphertext'].progress_apply(lambda x: str_digit_stat(x))\n",
    "\n",
    "print('Extracting features for train:')\n",
    "extract_features(X)\n",
    "train.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
