{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import gc\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import timeit\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.metrics import log_loss,confusion_matrix,roc_curve,auc\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20 newsgroups challenge problemi, machine learning açısından yıllarca kullanılan bilindik bir problemdir. Bu problemde 20 adet konu başlığı bulunmaktadır. Bu farklı gazete yazılarının hangi konu başlığına ait olduğunu bulmaya çalışılır. 20 newsgroups ciphertext challenge da ise başlıkta da anlaşılacağı üzere gazete yazıları şifrenlenmiştir. 4 farklı zorluk derecesine göre şifrelenen gazete yazıları sırasıyla 1,2,3,4 zorluk derecesine sahiptir. 1 zorluk derecesindeki yazılar 1 adet şifreleme geçirmiş, 2 zorluk derecesine sahip yazılar ise art arda 2 adet şifreleme geçirmiştir. Diğerleride aynı şekilde şifrelenmişlerdir.\n",
    "\n",
    "Machine learning alanında dil ile ilgili olan problemlere NLP (Neuro-linguistic programming) denir. Bu tür problemlerde yapılan başlıca adımlar vardır. Bunlardan bazıları şunlardır;\n",
    "\n",
    "1-)Yazıyı Temizlemek: \n",
    "- Alakasız karakterleri yazıdan silmek. Bunlara bazı gereksiz sayılar, noktalama işaretleri veya İngilizcedeki \"am\", \"is\", \"are\" gibi kelimeler dahildir.\n",
    "- Yazılardaki bütün harfleri küçüük harfe çevirmek çünkü “hello”, “Hello”, ve “HELLO” gibi kelimelerin hepsi aynı şeyi ifade ediyor.\n",
    "\n",
    "2-)Modellemek için kelimeleri birbirinden ayırmak\n",
    "- Modellemede kullanmak için yazı içindeki her kelimeyi ayrı bir eleman gibi düşünmek. Buna ingilizcede \"Tokenize\" deniyor. \n",
    "\n",
    "3-)Vectore çevirmek\n",
    "- Machine learning de kullanılan modeller kelime veya karakter bazlı olmadığı için yazıları sayıya çevirmek gerekiyor. Bunu yapmak içinde kelimeleri veya belli sayıda karakterleri çeşitli fonksiyonlardan geçirerek onların vektor karşılıklarıyla işlem yapılmalıdır. \n",
    "\n",
    "4-)Özellik çoğaltma\n",
    "- NLP problemlerinde genelde yazının yanında herhangi bir özellik verilmez. Vectore çevirme aslında bir nevi yazının özelliğini ortaya çıkartma olarak algılanabilir. Bunların yanında yazıların özelliklerini çıkartmak için python da kullanılan bazı yöntemler vardır. Bu yöntemler fuzzy,Levenshtein gibi yardımcı araçlarla ortaya çıkartılabilir.\n",
    "\n",
    "4-)Model bulmak\n",
    "- NLP için sıklıkla kullanılan bazı modelleme yöntemleri vardır. Bunlardan bazıları Naive-Bayes, SVM, Logistic Regression, Ensemble gibi modellemelerdir. Bunların her biri denenip hangisinin daha iyi sonuç veridiği bulunmalıdır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adım 1:\n",
    "\n",
    "\"c1|FaAO120O'8o\u0002vfoy1W#at\u001bvGs1[1s1[1/1]O-a8o1\" yazısı zorluk derecesi 1 olan şifrenlenmiş bir yazı örneğidir. Burada ilk olarak herhangi bir temizleme işlemi yapmadan yazılar vectorlere çevrilerek Naive-Bayes, SVM, Logistic Regression, RandomForest ve XGBClassifier modelleri denenerek sonuçlar analiz edilmiştir. Burada verinin büyüklüğü nedeniyle sadece zorluk derecesi 1 olan verilerle çalışılmıştır. \n",
    "\n",
    "Vectore çevirmek için ilk başta CountVectorizer fonksiyonu kullanılmıştır. Şifrelenmiş verilerde büyük küçük harf önemli olduğu için büyük harfler küçük harfe çevrilmemiştir. Tokinizer olarak ngram kullanılmıştır. Fonksiyon çıkışında her kelime ayrılıp bir vector olarak ifade edilmiştir. Vectorizer da en iyi parametreleri bulmak için GridSearch yöntemi kullanılmıştır. Burada analyzer parametresi olarak ('word','char'), ngram parametresi olarak birçok değer denemiştir. Bulunan en iyi sonuç ile yazılar vectorlere çevrilmiştir. Daha sonra modellere uygulanarak sırasıyla sonuçlar gözlemlenmiştir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DUZ DENEME\n",
    "df = pd.read_csv('20-newsgroups-ciphertext-challenge/train.csv')\n",
    "data_1 = df.query('difficulty==1')\n",
    "X = data_1.iloc[:,-2]\n",
    "y = data_1.iloc[:,-1]\n",
    "\n",
    "parameters = {\n",
    "    #'clf__alpha': (1.0000000000000001e-05, 9.9999999999999995e-07),\n",
    "    #'clf__max_iter': (10, 50, 80),\n",
    "    #'clf__penalty': ('l2', 'elasticnet'),\n",
    "    #'tfidf__use_idf': (True, False),\n",
    "    #'tfidf__sublinear_tf': (True, False),\n",
    "    #'tfidf__lowercase': (True,False),\n",
    "    #'tfidf__strip_accents': ('ascii','unicode'),\n",
    "    #'tfidf__analyzer': ('word','char'),\n",
    "    #'tfidf__ngram_range': ((1,4),(1,5)),\n",
    "    #'tfidf__max_features': (7500,5000),\n",
    "    #'svd__n_components': (500,1000),\n",
    "    #'vect__ngram_range': ((1, 6), (1, 7)),\n",
    "    #'vect__analyzer': ('word','char'),\n",
    "    #'vect__min_df': (0.001,0.01),\n",
    "}\n",
    "\n",
    "# build TFIDF Vectorizer\n",
    "tokens= ((u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b')) # makes sure it matches a word but contains at least one letter\n",
    "\n",
    "nb = Pipeline([('vect', CountVectorizer(analyzer = 'char',ngram_range=(1, 6))),\n",
    "               #('tfidf', TfidfVectorizer(strip_accents='ascii',analyzer='char',token_pattern=tokens,\n",
    "               #                          ngram_range=(1,4),lowercase=False,dtype=np.uint32,max_features=7500)),\n",
    "               #('svd', TruncatedSVD(algorithm='arpack')),\n",
    "               ('clf', MultinomialNB(alpha=1.0e-10)),])\n",
    "#grid_search = GridSearchCV(nb, parameters, cv=5,n_jobs=-1, verbose=1)\n",
    "\n",
    "lr = Pipeline([#('vect', CountVectorizer(analyzer = 'word',ngram_range=(1, 2))),\n",
    "               ('tfidf', TfidfVectorizer(strip_accents='ascii',analyzer='word',token_pattern=tokens,\n",
    "                                         ngram_range=(1,2),lowercase=False,dtype=np.float32,max_features=7500)),\n",
    "               ('clf', LogisticRegression(solver='saga', n_jobs=-1, C=1e5, multi_class='auto')),])\n",
    "#grid_search = GridSearchCV(lr, parameters, cv=5,n_jobs=-1, verbose=1)\n",
    "\n",
    "svm = Pipeline([#('vect', CountVectorizer(analyzer = 'char',ngram_range=(1, 6))),\n",
    "                ('tfidf', TfidfVectorizer(strip_accents='ascii',analyzer='char',token_pattern=tokens,\n",
    "                                         ngram_range=(1,4),lowercase=False,dtype=np.float32,max_features=7500)),\n",
    "                ('clf', SGDClassifier(loss='hinge',penalty='l2',alpha=1e-5,random_state=0,max_iter=1000, \n",
    "                                     tol=1e-3,n_jobs=-1)),])\n",
    "#grid_search = GridSearchCV(svm, parameters, cv=5,n_jobs=-1, verbose=1)\n",
    "\n",
    "rf = Pipeline([#('vect', CountVectorizer(analyzer = 'char',ngram_range=(1, 6))),\n",
    "               ('tfidf', TfidfVectorizer(strip_accents='ascii',analyzer='char',token_pattern=tokens,\n",
    "                                         ngram_range=(1,4),lowercase=False,dtype=np.float32,max_features=7500)), \n",
    "               ('clf', RandomForestClassifier(n_estimators=500,max_features='log2',min_samples_split=4)),])\n",
    "#grid_search = GridSearchCV(rf, parameters, cv=5,n_jobs=-1, verbose=1)\n",
    "\n",
    "xgb = Pipeline([#('vect', CountVectorizer(analyzer = 'char',ngram_range=(1, 6))),\n",
    "                ('tfidf', TfidfVectorizer(strip_accents='ascii',analyzer='char',token_pattern=tokens,\n",
    "                                         ngram_range=(1,4),lowercase=False,dtype=np.float32,max_features=7500)),\n",
    "                ('clf', XGBClassifier()),])\n",
    "#grid_search = GridSearchCV(xgb, parameters, cv=5,n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 -)Naive-Bayes train score= 0.9938674321503131\n",
    "1 -)Naive-Bayes test score= 0.6046753246753247\n",
    "2 -)Naive-Bayes train score= 0.9937426671881111\n",
    "2 -)Naive-Bayes test score= 0.6235662148070907\n",
    "3 -)Naive-Bayes train score= 0.9934827945776851\n",
    "3 -)Naive-Bayes test score= 0.6061554512258738\n",
    "4 -)Naive-Bayes train score= 0.9930926625830835\n",
    "4 -)Naive-Bayes test score= 0.6132567849686847\n",
    "5 -)Naive-Bayes train score= 0.9936164669098488\n",
    "5 -)Naive-Bayes test score= 0.6121275483533717\n",
    "Elapsed time 114.89954035500705\n",
    "\n",
    "1 -)Naive-Bayes train score= 0.6860647181628392\n",
    "1 -)Naive-Bayes test score= 0.41818181818181815\n",
    "2 -)Naive-Bayes train score= 0.6782688045887107\n",
    "2 -)Naive-Bayes test score= 0.4191866527632951\n",
    "3 -)Naive-Bayes train score= 0.6904327424400417\n",
    "3 -)Naive-Bayes test score= 0.41366718831507565\n",
    "4 -)Naive-Bayes train score= 0.6881271992701681\n",
    "4 -)Naive-Bayes test score= 0.43893528183716074\n",
    "5 -)Naive-Bayes train score= 0.6827774882751433\n",
    "5 -)Naive-Bayes test score= 0.42812336644014637\n",
    "Elapsed time 58.729096541996114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -)Naive-Bayes train score= 0.9821242171189979\n",
      "1 -)Naive-Bayes test score= 0.49246753246753244\n",
      "2 -)Naive-Bayes train score= 0.9813583626645809\n",
      "2 -)Naive-Bayes test score= 0.4984358706986444\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-195-323e5c87ac8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mskf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Naive-Bayes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"-)Naive-Bayes train score=\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \"\"\"\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    228\u001b[0m                 Xt, fitted_transformer = fit_transform_one_cached(\n\u001b[1;32m    229\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                     **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, **fit_params)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_fit_transform_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1032\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    942\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    945\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfeature_idx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_counter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m                         \u001b[0mfeature_counter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# trigger timer\n",
    "start_time = timeit.default_timer()\n",
    "i=1\n",
    "# create and train model\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "for train, test in skf.split(X, y):\n",
    "    # Naive-Bayes\n",
    "    nb.fit(X.iloc[train], y.iloc[train])\n",
    "    y_pred = nb.predict(X.iloc[test])\n",
    "    print(i,\"-)Naive-Bayes train score=\", nb.score(X.iloc[train],y.iloc[train]))\n",
    "    print(i,\"-)Naive-Bayes test score=\", accuracy_score(y_pred, y.iloc[test]))  \n",
    "    i=i+1\n",
    "\n",
    "#print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "#print(\"Best parameters set:\")\n",
    "#best_parameters = grid_search.best_estimator_.get_params()\n",
    "#for param_name in sorted(parameters.keys()):\n",
    "#    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "# calculate time interval\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(\"Elapsed time\", elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 -)Logistic Regression Classifier train score= 0.9973387601753287\n",
    "1 -)Logistic Regression Classifier test score= 0.42018119337706966\n",
    "2 -)Logistic Regression Classifier train score= 0.9976533166458073\n",
    "2 -)Logistic Regression Classifier test score= 0.4350954019393181\n",
    "3 -)Logistic Regression Classifier train score= 0.9970303219756174\n",
    "3 -)Logistic Regression Classifier test score= 0.4261986837981824\n",
    "Elapsed time 65.17152920400258\n",
    "\n",
    "1 -)Logistic Regression Classifier train score= 0.993894802755166\n",
    "1 -)Logistic Regression Classifier test score= 0.44204935957513275\n",
    "2 -)Logistic Regression Classifier train score= 0.9937421777221527\n",
    "2 -)Logistic Regression Classifier test score= 0.44541757898029405\n",
    "3 -)Logistic Regression Classifier train score= 0.9931228508909034\n",
    "3 -)Logistic Regression Classifier test score= 0.43685365089313694\n",
    "Elapsed time 32.350123803000315"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uzun/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -)Logistic Regression Classifier train score= 0.993894802755166\n",
      "1 -)Logistic Regression Classifier test score= 0.44204935957513275\n",
      "2 -)Logistic Regression Classifier train score= 0.9937421777221527\n",
      "2 -)Logistic Regression Classifier test score= 0.44541757898029405\n",
      "3 -)Logistic Regression Classifier train score= 0.9931228508909034\n",
      "3 -)Logistic Regression Classifier test score= 0.43685365089313694\n",
      "Elapsed time 32.350123803000315\n"
     ]
    }
   ],
   "source": [
    "# trigger timer\n",
    "start_time = timeit.default_timer()\n",
    "i=1\n",
    "# create and train model\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "for train, test in skf.split(X, y):\n",
    "    # Logistic Regression Classifier\n",
    "    lr.fit(X.iloc[train], y.iloc[train])\n",
    "    y_pred = lr.predict(X.iloc[test])\n",
    "    print(i,\"-)Logistic Regression Classifier train score=\", lr.score(X.iloc[train],y.iloc[train]))\n",
    "    print(i,\"-)Logistic Regression Classifier test score=\", accuracy_score(y_pred, y.iloc[test]))\n",
    "    i=i+1\n",
    "    \n",
    "# calculate time interval\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(\"Elapsed time\", elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 -)SVM train score= 0.9616468378209142\n",
    "1 -)SVM test score= 0.5029678225554515\n",
    "2 -)SVM train score= 0.9586983729662077\n",
    "2 -)SVM test score= 0.5176728182671254\n",
    "3 -)SVM train score= 0.9477961863082213\n",
    "3 -)SVM test score= 0.4992165465371357\n",
    "Elapsed time 141.1674568860035\n",
    "\n",
    "1 -)SVM train score= 0.9575767063243582\n",
    "1 -)SVM test score= 0.5301468291159013\n",
    "2 -)SVM train score= 0.949468085106383\n",
    "2 -)SVM test score= 0.5317485142320926\n",
    "3 -)SVM train score= 0.9554548296342608\n",
    "3 -)SVM test score= 0.5352554058288937\n",
    "Elapsed time 43.7149073959954"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -)SVM train score= 0.9575767063243582\n",
      "1 -)SVM test score= 0.5301468291159013\n",
      "2 -)SVM train score= 0.949468085106383\n",
      "2 -)SVM test score= 0.5317485142320926\n",
      "3 -)SVM train score= 0.9554548296342608\n",
      "3 -)SVM test score= 0.5352554058288937\n",
      "Elapsed time 43.7149073959954\n"
     ]
    }
   ],
   "source": [
    "# trigger timer\n",
    "start_time = timeit.default_timer()\n",
    "i=1\n",
    "# create and train model\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "for train, test in skf.split(X, y):\n",
    "    # SGDClassifier\n",
    "    svm.fit(X.iloc[train], y.iloc[train])\n",
    "    y_pred = svm.predict(X.iloc[test])\n",
    "    print(i,\"-)SVM train score=\", svm.score(X.iloc[train],y.iloc[train]))\n",
    "    print(i,\"-)SVM test score=\", accuracy_score(y_pred, y.iloc[test]))\n",
    "    i=i+1\n",
    "    \n",
    "# calculate time interval\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(\"Elapsed time\", elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 -)Random Forest train score= 0.9993738259236068\n",
    "1 -)Random Forest test score= 0.4957825679475164\n",
    "2 -)Random Forest train score= 0.9992177722152691\n",
    "2 -)Random Forest test score= 0.5032843290584923\n",
    "3 -)Random Forest train score= 0.9990622069396686\n",
    "3 -)Random Forest test score= 0.5155123785647132\n",
    "Elapsed time 514.6145224819993\n",
    "\n",
    "1 -)Random Forest train score= 0.9996869129618033\n",
    "1 -)Random Forest test score= 0.4126835363948766\n",
    "2 -)Random Forest train score= 0.9998435544430538\n",
    "2 -)Random Forest test score= 0.4119487019080388\n",
    "3 -)Random Forest train score= 0.9996874023132228\n",
    "3 -)Random Forest test score= 0.4202444374804137\n",
    "Elapsed time 131.42851219599834"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -)Random Forest train score= 0.9996869129618033\n",
      "1 -)Random Forest test score= 0.4126835363948766\n",
      "2 -)Random Forest train score= 0.9998435544430538\n",
      "2 -)Random Forest test score= 0.4119487019080388\n",
      "3 -)Random Forest train score= 0.9996874023132228\n",
      "3 -)Random Forest test score= 0.4202444374804137\n",
      "Elapsed time 131.42851219599834\n"
     ]
    }
   ],
   "source": [
    "# trigger timer\n",
    "start_time = timeit.default_timer()\n",
    "i=1\n",
    "# create and train model\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "for train, test in skf.split(X, y):\n",
    "    # Random Forest\n",
    "    rf.fit(X.iloc[train], y.iloc[train])\n",
    "    y_pred = rf.predict(X.iloc[test])\n",
    "    print(i,\"-)Random Forest train score=\", rf.score(X.iloc[train],y.iloc[train]))\n",
    "    print(i,\"-)Random Forest test score=\", accuracy_score(y_pred, y.iloc[test]))\n",
    "    i=i+1\n",
    "    \n",
    "# calculate time interval\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(\"Elapsed time\", elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-1a8c313ce915>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mskf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# XGBClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"-)XGBClassifier train score=\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    711\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1110\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# trigger timer\n",
    "start_time = timeit.default_timer()\n",
    "i=1\n",
    "# create and train model\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "for train, test in skf.split(X, y):\n",
    "    # XGBClassifier\n",
    "    xgb.fit(X.iloc[train], y.iloc[train])\n",
    "    y_pred = xgb.predict(X.iloc[test])\n",
    "    print(i,\"-)XGBClassifier train score=\", xgb.score(X.iloc[train],y.iloc[train]))\n",
    "    print(i,\"-)XGBClassifier test score=\", accuracy_score(y_pred, y.iloc[test]))\n",
    "    i=i+1\n",
    "    \n",
    "# calculate time interval\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(\"Elapsed time\", elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.25 µs\n",
      "Modeling..\n",
      "0.9311216429699842\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cross_val_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-199-3a86774915d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_tfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Auc Score: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_tfidf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cross_val_score' is not defined"
     ]
    }
   ],
   "source": [
    "X = X.astype(str)\n",
    "\n",
    "def tokenize(text): \n",
    "    return text.split(\"1\")\n",
    "\n",
    "def trimm(text):\n",
    "    return ' '.join([i for i in text if len(i) > 3])\n",
    "\n",
    "token_data = [tokenize(i) for i in X]\n",
    "X = [trimm(i) for i in token_data]\n",
    "\n",
    "# build TFIDF Vectorizer\n",
    "tokens= ((u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b')) # makes sure it matches a word but contains at least one letter\n",
    "\n",
    "word_vectorizer = TfidfVectorizer(sublinear_tf=True,strip_accents='ascii',analyzer='word',token_pattern=tokens,\n",
    "                                  ngram_range=(1,2),dtype=np.float32,max_features=7500)\n",
    "\n",
    "# Character Stemmer\n",
    "char_vectorizer = TfidfVectorizer(sublinear_tf=True,strip_accents='ascii',analyzer='char',token_pattern=tokens,\n",
    "                                  ngram_range=(2, 4),dtype=np.float32,max_features=12000)\n",
    "\n",
    "word_vectorizer.fit(X)\n",
    "char_vectorizer.fit(X)\n",
    "\n",
    "train_word_features = word_vectorizer.transform(X)\n",
    "train_char_features = char_vectorizer.transform(X)\n",
    "\n",
    "train_features = hstack([train_char_features,train_word_features])\n",
    "\n",
    "%time\n",
    "print(\"Modeling..\")\n",
    "\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(train_features, y, test_size=0.33)\n",
    "\n",
    "lr = LogisticRegression(solver=\"sag\", max_iter=100,class_weight='balanced',C=2.65,penalty='l2')\n",
    "lr.fit(train_features,y)\n",
    "lr_pred=lr.predict(X_test_tfidf)\n",
    "\n",
    "accuracy_tfidf =accuracy_score(y_test_tfidf,lr_pred)\n",
    "\n",
    "print(accuracy_tfidf)\n",
    "print(classification_report(y_test_tfidf,lr_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    10024\n",
      "4     9970\n",
      "1     9589\n",
      "3     9469\n",
      "Name: difficulty, dtype: int64\n",
      "target       0    1    2    3    4    5    6    7    8    9    10   11   12  \\\n",
      "difficulty                                                                    \n",
      "1           420  465  360  346  320  466  361  331  380  486  540  563  344   \n",
      "2           455  528  652  391  315  550  310  405  387  301  580  651  396   \n",
      "3           394  567  695  386  313  524  293  422  331  460  479  534  338   \n",
      "4           465  366  940  382  326  653  272  437  386  413  437  622  429   \n",
      "\n",
      "target       13   14   15   16   17   18   19  \n",
      "difficulty                                     \n",
      "1           482  576  765  471  834  675  404  \n",
      "2           500  566  692  536  912  547  350  \n",
      "3           593  470  513  506  714  553  384  \n",
      "4           599  463  582  589  693  539  377  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "c     53752\n",
       "1    505426\n",
       "|      7490\n",
       "F      6134\n",
       "a    121319\n",
       "A     58440\n",
       "O    134813\n",
       "2      8502\n",
       "0     75960\n",
       "'     34490\n",
       "dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dataframe\n",
    "df = pd.read_csv('20-newsgroups-ciphertext-challenge/train.csv')\n",
    "#test = pd.read_csv('20-newsgroups-ciphertext-challenge/test.csv')\n",
    "\n",
    "# dataframe for only difficulty=1\n",
    "data1 = df.query('difficulty==1')\n",
    "# how many elements do have diffuculty class\n",
    "print(df['difficulty'].value_counts()) \n",
    "# count repeated chars in ciphertext class\n",
    "alp = pd.Series(Counter(''.join(data1['ciphertext'])))  \n",
    "print(pd.crosstab(df['difficulty'], df['target']))\n",
    "alp.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text): \n",
    "    return text.split(\"1\")\n",
    "\n",
    "def trimm(text):\n",
    "    return ' '.join([i for i in text if len(i) > 1])\n",
    "\n",
    "token_data = [tokenize(i) for i in X]\n",
    "X = [trimm(i) for i in token_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9589, 10)\n"
     ]
    }
   ],
   "source": [
    "# PCA \n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "a=cosine_similarity(train_word_features, train_word_features)\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "\n",
    "principalComponents = pca.fit_transform(a)\n",
    "\n",
    "principalDf = pd.DataFrame(data = principalComponents)\n",
    "print(principalDf.shape)\n",
    "#finalDf = pd.concat([principalDf, y], axis = 1)\n",
    "#finalDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features for train:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "count_chars: 100%|██████████| 39052/39052 [00:09<00:00, 4323.13it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:06<00:00, 5994.40it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:05<00:00, 7667.39it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:01<00:00, 32674.29it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:00<00:00, 339877.12it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:06<00:00, 5800.36it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:04<00:00, 7963.75it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:01<00:00, 32945.49it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:00<00:00, 594070.61it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:07<00:00, 5549.92it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:04<00:00, 8632.10it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:01<00:00, 34764.83it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:00<00:00, 613254.41it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:06<00:00, 6188.96it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:04<00:00, 8141.06it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:01<00:00, 34240.53it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:00<00:00, 555007.25it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:06<00:00, 6324.34it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:04<00:00, 8091.41it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:01<00:00, 33223.52it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:00<00:00, 559030.58it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:01<00:00, 23916.42it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:01<00:00, 30142.55it/s]\n",
      "distances: 100%|██████████| 39052/39052 [00:00<00:00, 90537.07it/s]\n",
      "strstat: 100%|██████████| 39052/39052 [00:28<00:00, 1382.72it/s]\n",
      "str_digit_stat: 100%|██████████| 39052/39052 [00:27<00:00, 1425.88it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>ciphertext</th>\n",
       "      <th>target</th>\n",
       "      <th>nunique</th>\n",
       "      <th>len</th>\n",
       "      <th>n_l</th>\n",
       "      <th>n_n</th>\n",
       "      <th>n_s</th>\n",
       "      <th>n_ul</th>\n",
       "      <th>...</th>\n",
       "      <th>str_max</th>\n",
       "      <th>str_skew</th>\n",
       "      <th>str_kurtosis</th>\n",
       "      <th>str_digit_sum</th>\n",
       "      <th>str_digit_mean</th>\n",
       "      <th>str_digit_std</th>\n",
       "      <th>str_digit_min</th>\n",
       "      <th>str_digit_max</th>\n",
       "      <th>str_digit_skew</th>\n",
       "      <th>str_digit_kurtosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_88b9bbd73</td>\n",
       "      <td>4</td>\n",
       "      <td>ob|I\u0006\f",
       "K?zzhX*L{83B3Z,\u0006FuL*Pusm$83L\\t@r$$*38,8s...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>300</td>\n",
       "      <td>0.473333</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.356667</td>\n",
       "      <td>0.246667</td>\n",
       "      <td>...</td>\n",
       "      <td>127.0</td>\n",
       "      <td>-0.091650</td>\n",
       "      <td>-0.950683</td>\n",
       "      <td>2664.0</td>\n",
       "      <td>52.235294</td>\n",
       "      <td>3.299366</td>\n",
       "      <td>48.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>-0.001394</td>\n",
       "      <td>-1.549887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_f489bd59f</td>\n",
       "      <td>1</td>\n",
       "      <td>c1|FaAO120O'8o\u0002vfoy1W#at\u001bvGs1[1s1[1/1]O-a8o1-\u001b...</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>300</td>\n",
       "      <td>0.383333</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.376667</td>\n",
       "      <td>0.103333</td>\n",
       "      <td>...</td>\n",
       "      <td>124.0</td>\n",
       "      <td>-0.047954</td>\n",
       "      <td>-1.144330</td>\n",
       "      <td>3568.0</td>\n",
       "      <td>49.555556</td>\n",
       "      <td>2.146631</td>\n",
       "      <td>48.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>2.564841</td>\n",
       "      <td>4.938975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_f90fee9c7</td>\n",
       "      <td>2</td>\n",
       "      <td>1*e4N8$f$0ccOui\u0018hkHek\u001a$\u0010k*\u001aV*hoe\u0010V\u001a$Hj8\bV\u0003hH8...</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>300</td>\n",
       "      <td>0.430000</td>\n",
       "      <td>0.146667</td>\n",
       "      <td>0.423333</td>\n",
       "      <td>0.176667</td>\n",
       "      <td>...</td>\n",
       "      <td>127.0</td>\n",
       "      <td>-0.063138</td>\n",
       "      <td>-1.231967</td>\n",
       "      <td>2406.0</td>\n",
       "      <td>54.681818</td>\n",
       "      <td>2.274091</td>\n",
       "      <td>48.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>-1.587334</td>\n",
       "      <td>1.299930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_8303ced65</td>\n",
       "      <td>1</td>\n",
       "      <td>O8v^10\u001bO#to1'#^'^\u0002\u001b\u0002tv1^]s111t0\u001b1O\u0002taq&gt;\u001b-ata_1...</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>300</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.246667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.153333</td>\n",
       "      <td>...</td>\n",
       "      <td>125.0</td>\n",
       "      <td>-0.077411</td>\n",
       "      <td>-0.983794</td>\n",
       "      <td>3682.0</td>\n",
       "      <td>49.756757</td>\n",
       "      <td>2.276857</td>\n",
       "      <td>48.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>2.245122</td>\n",
       "      <td>3.317398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_72abc2cb7</td>\n",
       "      <td>2</td>\n",
       "      <td>e\u001aV}H\u001a}kh\u001afe4b8'S.Vc}{A\f",
       "\f",
       ".#VikV.\u001afV?{$f7\u001a$Hjb8...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>300</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.113333</td>\n",
       "      <td>0.453333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>...</td>\n",
       "      <td>126.0</td>\n",
       "      <td>-0.241807</td>\n",
       "      <td>-1.200268</td>\n",
       "      <td>1875.0</td>\n",
       "      <td>55.147059</td>\n",
       "      <td>1.477923</td>\n",
       "      <td>51.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>-1.675256</td>\n",
       "      <td>1.324605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Id  difficulty  \\\n",
       "0  ID_88b9bbd73           4   \n",
       "1  ID_f489bd59f           1   \n",
       "2  ID_f90fee9c7           2   \n",
       "3  ID_8303ced65           1   \n",
       "4  ID_72abc2cb7           2   \n",
       "\n",
       "                                          ciphertext  target  nunique  len  \\\n",
       "0  ob|I\u0006\n",
       "K?zzhX*L{83B3Z,\u0006FuL*Pusm$83L\\t@r$$*38,8s...      10        1  300   \n",
       "1  c1|FaAO120O'8o\u0002vfoy1W#at\u001bvGs1[1s1[1/1]O-a8o1-\u001b...      13        1  300   \n",
       "2  1*e4N8$f$0ccOui\u0018hkHek\u001a$\u0010k*\u001aV*hoe\u0010V\u001a$Hj8\bV\u0003hH8...      19        1  300   \n",
       "3  O8v^10\u001bO#to1'#^'^\u0002\u001b\u0002tv1^]s111t0\u001b1O\u0002taq>\u001b-ata_1...      17        1  300   \n",
       "4  e\u001aV}H\u001a}kh\u001afe4b8'S.Vc}{A\n",
       "\n",
       ".#VikV.\u001afV?{$f7\u001a$Hjb8...       0        1  300   \n",
       "\n",
       "        n_l       n_n       n_s      n_ul         ...          str_max  \\\n",
       "0  0.473333  0.170000  0.356667  0.246667         ...            127.0   \n",
       "1  0.383333  0.240000  0.376667  0.103333         ...            124.0   \n",
       "2  0.430000  0.146667  0.423333  0.176667         ...            127.0   \n",
       "3  0.420000  0.246667  0.333333  0.153333         ...            125.0   \n",
       "4  0.433333  0.113333  0.453333  0.133333         ...            126.0   \n",
       "\n",
       "   str_skew  str_kurtosis  str_digit_sum  str_digit_mean  str_digit_std  \\\n",
       "0 -0.091650     -0.950683         2664.0       52.235294       3.299366   \n",
       "1 -0.047954     -1.144330         3568.0       49.555556       2.146631   \n",
       "2 -0.063138     -1.231967         2406.0       54.681818       2.274091   \n",
       "3 -0.077411     -0.983794         3682.0       49.756757       2.276857   \n",
       "4 -0.241807     -1.200268         1875.0       55.147059       1.477923   \n",
       "\n",
       "   str_digit_min  str_digit_max  str_digit_skew  str_digit_kurtosis  \n",
       "0           48.0           57.0       -0.001394           -1.549887  \n",
       "1           48.0           56.0        2.564841            4.938975  \n",
       "2           48.0           57.0       -1.587334            1.299930  \n",
       "3           48.0           56.0        2.245122            3.317398  \n",
       "4           51.0           56.0       -1.675256            1.324605  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature extraction\n",
    "import datetime\n",
    "import gc\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.stats import skew, kurtosis\n",
    "import lightgbm as lgb\n",
    "\n",
    "import Levenshtein\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "'''\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(trainDF['text'])\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(trainDF['text'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "\n",
    "ensemble.RandomForestClassifier()\n",
    "\n",
    "diff1['ciphertext'] = diff1['ciphertext'].apply(lambda x: x.replace('1', ' '))\n",
    "diff2['ciphertext'] = diff2['ciphertext'].apply(lambda x: x.replace('8', ' '))\n",
    "diff3['ciphertext'] = diff3['ciphertext'].apply(lambda x: x.replace('8', ' '))\n",
    "diff4['ciphertext'] = diff4['ciphertext'].apply(lambda x: x.replace('8', ' '))\n",
    "'''\n",
    "\n",
    "def extract_features(df):\n",
    "    #df['nunique'] = df['ciphertext'].apply(lambda x: len(np.unique(x)))\n",
    "    #df['len'] = df['ciphertext'].apply(lambda x: len(x))\n",
    "\n",
    "    def count_chars(x):\n",
    "        n_l = 0 # count letters\n",
    "        n_n = 0 # count numbers\n",
    "        n_s = 0 # count symbols\n",
    "        n_ul = 0 # count upper letters\n",
    "        n_ll = 0 # count lower letters\n",
    "        for i in range(0, len(x)):\n",
    "            if x[i].isalpha():\n",
    "                n_l += 1\n",
    "                if x[i].isupper():\n",
    "                    n_ul += 1\n",
    "                elif x[i].islower():\n",
    "                    n_ll += 1\n",
    "            elif x[i].isdigit():\n",
    "                n_n += 1\n",
    "            else:\n",
    "                n_s += 1\n",
    "\n",
    "        return pd.Series([n_l, n_n, n_s, n_ul, n_ll])\n",
    "\n",
    "    cols = ['n_l', 'n_n', 'n_s', 'n_ul', 'n_ll']\n",
    "    for c in cols:\n",
    "        df[c] = 0\n",
    "    tqdm.pandas(desc='count_chars')\n",
    "    df[cols] = df['ciphertext'].progress_apply(lambda x: count_chars(x))\n",
    "    for c in cols:\n",
    "        df[c] /= df['len']\n",
    "\n",
    "    tqdm.pandas(desc='distances')\n",
    "    df['Levenshtein_distance'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.distance(x, x[::-1]))\n",
    "    df['Levenshtein_ratio'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.ratio(x, x[::-1]))\n",
    "    df['Levenshtein_jaro'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.jaro(x, x[::-1]))\n",
    "    df['Levenshtein_hamming'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.hamming(x, x[::-1]))\n",
    "\n",
    "    for m in range(1, 5):\n",
    "        df['Levenshtein_distance_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.distance(x[:-m], x[m:]))\n",
    "        df['Levenshtein_ratio_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.ratio(x[:-m], x[m:]))\n",
    "        df['Levenshtein_jaro_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.jaro(x[:-m], x[m:]))\n",
    "        df['Levenshtein_hamming_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.hamming(x[:-m], x[m:]))\n",
    "    \n",
    "    df['Levenshtein_distance_h'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.distance(x[:len(x)//2], x[len(x)//2:]))\n",
    "    df['Levenshtein_ratio_h'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.ratio(x[:len(x)//2], x[len(x)//2:]))\n",
    "    df['Levenshtein_jaro_h'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.jaro(x[:len(x)//2], x[len(x)//2:]))\n",
    "    \n",
    "    # All symbols stats\n",
    "    def strstat(x):\n",
    "        r = np.array([ord(c) for c in x])\n",
    "        return pd.Series([\n",
    "            np.sum(r), \n",
    "            np.mean(r), \n",
    "            np.std(r), \n",
    "            np.min(r), \n",
    "            np.max(r),\n",
    "            skew(r), \n",
    "            kurtosis(r),\n",
    "            ])\n",
    "    cols = ['str_sum', 'str_mean', 'str_std', 'str_min', 'str_max', 'str_skew', 'str_kurtosis']\n",
    "    for c in cols:\n",
    "        df[c] = 0\n",
    "    tqdm.pandas(desc='strstat')\n",
    "    df[cols] = df['ciphertext'].progress_apply(lambda x: strstat(x))\n",
    "    \n",
    "    # Digit stats\n",
    "    def str_digit_stat(x):\n",
    "        r = np.array([ord(c) for c in x if c.isdigit()])\n",
    "        if len(r) == 0:\n",
    "            r = np.array([0])\n",
    "        return pd.Series([\n",
    "            np.sum(r), \n",
    "            np.mean(r), \n",
    "            np.std(r), \n",
    "            np.min(r), \n",
    "            np.max(r),\n",
    "            skew(r), \n",
    "            kurtosis(r),\n",
    "            ])\n",
    "    cols = ['str_digit_sum', 'str_digit_mean', 'str_digit_std', 'str_digit_min', \n",
    "        'str_digit_max', 'str_digit_skew', 'str_digit_kurtosis']\n",
    "    for c in cols:\n",
    "        df[c] = 0\n",
    "    tqdm.pandas(desc='str_digit_stat')\n",
    "    df[cols] = df['ciphertext'].progress_apply(lambda x: str_digit_stat(x))\n",
    "\n",
    "print('Extracting features for train:')\n",
    "extract_features(train)\n",
    "train.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
